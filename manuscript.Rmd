---
title: "Opinions on Hard-to-Discuss Topics Change More via Cohort Replacement"
author: 
  - Nicolas Restrepo Ochoa[^NRO]
  - Stephen Vaisey[^SV]
output:
  pdf_document:
  html_document:
    toc: yes
    theme: united
header-includes:
- \usepackage{setspace}
- \doublespacing
- \usepackage{lineno}
- \linenumbers
- \def\linenumberfont{\normalfont\tiny\sffamily}
bibliography: sensitive_issues.bib
abstract: "Cohort replacement---the replacement of older cohorts by their successors who developed under different conditions---is an important process for creating cultural change. Research on public opinion in the United States indicates that most aggregate change is best understood as the result of cohort replacement, rather than of individuals changing their minds.  However, some publicly salient sensitive issues, like gay rights, appear to be exceptions. Why exactly these issues have different patterns of change is not well understood. A key reason is that previous work is limited by its focus on a single national context and the lack of a systematic comparison between more and less sensitive issues. We use data from the 1981-2020 World Values Surveys to compare aggregate changes in public opinion in 8 countries. We compare the trajectories of more sensitive and less sensitive issues to see if the pattern observed in the United States is generalizable. We find common trends-–-like changes in attitudes towards homosexuality-–-across most countries, as well as context-specific patterns that follow recognizable historical processes. Our key finding is that sensitive issues seem to change more through cohort replacement. In short, issues that are difficult to talk about change more privately."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F)

library(tidyverse)
library(countrycode)
library(janitor)
library(splines)
library(performance)
library(ggrepel)
library(hrbrthemes)
library(patchwork)
library(broom)
library(broom.mixed)
library(gtsummary)
library(labelled)
library(brms)
theme_set(theme_bw())
```

[^NRO]: Anthropology Department, Univerity of California at Davis.  
[^SV]: Sociology Department, Duke University.

Words: `r wordcountaddin::word_count("manuscript.Rmd")`

\newpage

## Introduction

Much of the scholarly interest in culture lies in trying to understand how it changes. Cultural trends like secularization [@bergerSecularizationDesecularization2002; @chavesSecularizationDecliningReligious1994] or the rise of (and backlash against) gender equality [@velascoTransnationalBacklashDeinstitutionalization2023] continue to receive sustained academic attention. Recent work on opinion change in the U.S. suggests that these changes happen primarily through cohort replacement [@vaiseyCulturalFragmentationAcquired2016a; @kileyMeasuringStabilityChange2020]. In other words, culture changes as young people, raised under different conditions, replace those that came before them. Nonetheless, some salient issues, like attitudes about homosexuality, seem to be exceptions, where individuals exhibit considerable durable change beyond adulthood [@kileyMeasuringStabilityChange2020]. This suggests that certain cultural issues might change in response to different underlying mechanisms. This is precisely what we explore in this paper.

We have two main goals in this article. First, we will examine whether there are systematic differences in the patterns of change across different variables, especially in relation to how sensitive---that is, how difficult to discuss---they are considered to be. We have evidence that individuals do update their beliefs about particularly salient issues. However, these issues tend to be also hotly debated, and difficult to talk about, where we expect interlocutors to be firmly entrenched in their beliefs. The sensitivity of an topic might be a useful gateway to start asking how different variables change through varying mechanisms. Second, we want to move beyond U.S. data and examine cross-cultural variation in how different issues change. In short, we want to dig deeper into the mechanisms that underpin change for different cultural issues, and we want to examine whether these mechanisms of change are consistent across contexts.

We investigate these questions use data from the World Value Survey (WVS) [@inglehartWorldValuesSurveys2000]. Though the data is cross-sectional and therefore cannot answer any questions about individual-level change, we can use it to adjudicate between different mechanisms that might account for population-level cultural change. To do this, we propose a straightforward method[^1]. We begin by positing an idealized model, where, after the critical period of youth, individuals do not change their beliefs. Under these assumptions, all cultural change can be explained by *between-cohort differences* and all we need to know to estimate a person's opinion on a given issue is to know their year of birth. Then, we consider another model, where the average opinion of each cohort is allowed to change linearly over time. We contend that, when fitted to the WVS data, the relative explanatory power of these two models provides an indication of the mechanism that might be responsible for change for a given issue. If the proportion of the variance explained is relatively unchanged when we move from the first model to the second one, then this suggests a given issue is changing primarily through cohort replacement. If the second model improves greatly on the first, then this would point towards the importance of *within-cohort change* in accounting for the trends.

[^1]: All necessary code and data to reproduce this paper is available on: <https://github.com/NicolasRestrep/sensitive_change> .

To examine patterns of change across different types of cultural issues, we fit these models to 56 different variables, measured between 1981-2020, across 8 countries. We select the countries -- Argentina, Australia, Canada, Japan, Mexico, South Africa, Sweden, and the USA -- based on completeness, seeking to cover the longest time-spans possible with this survey. We also choose to focus on variables that have been asked in all the waves of the WVS, and that cover a wide-range of topics and different levels of sensitivity, from the justifiability of euthanasia to whether imagination is a desirable attribute in children.

Our results produce several key insights. We show that, consistent with previous work on opinion change over the past four or five decades, large differences are uncommon. Nonetheless, just like work on attitudinal change in the U.S. [@kileyMeasuringStabilityChange2020], we find that the variable that has changed most consistently across the countries is related to attitudes about homosexuality. Furthermore, we show that for the variables that change the most, a considerable proportion of this variation can be attributed to cohort replacement. Perhaps most relevant to our questions about the mechanisms of change, we see a pattern across the sensitivity of different cultural issues. We find that change in more sensitive topics can be explained mostly by *between-cohort differences*, and variation in less sensitive issues can be attributed more to *within-cohort change*. This provides some preliminary evidence for the idea that different issues do change through varying mechanisms.

### Cultural change and its elements

Cultural change has been a central preoccupation of social scientists. Most social theories seek to address, to different extents, how societies change and evolve, and often the first criticism that is leveled at any account of the social world is whether it is capable of *"accounting for change"* [@martinThinkingTheory2015]. Recently, as new data sources with longer time series have come available, there has been a renewed interest in trying to tackle cultural change. This newer work has focused on linking individual mechanisms of attitudinal updating to large-scale processes of change [@kileyMeasuringStabilityChange2020; @keskinturkReligiousBeliefAlignment2021; @bartelsGenerationalModelPolitical2014]. When it comes to matters of opinion, *societies* do not change; *individuals* change, as does the composition of individuals in a population. The aggregation of those individual processes of updating is what can be measured as shifts at the population level. This is not just pretentious wordplay, but a fundamental point in the study of social change: theories of large-scale cultural change are -- at their core -- accounts of how individuals update their beliefs and habits.

Large-scale change can occur through several individual-level mechanisms. Individuals might tend to update their beliefs in a particular way as they age. For instance, they might veer away from direct action and radical politics as they accrue wealth and have more to lose in the case of a structural societal change [@mcadamBiographicalConsequencesActivism1989]. The type of trends we expect to happen throughout an individual's lifetime are often described as *age effects* [@fosseAnalyzingAgeperiodcohortData2019]. Large-scale change might also happen as new information or events affect a whole cross-section of the population. During a time for war, for example, we might expect individuals -- across all age-groups -- to shift in their sentiment towards the armed forces or their country in general. These are what are known as *period effects* [@fosseAnalyzingAgeperiodcohortData2019]. It is also possible to think that certain historical moments leave a mark on those individuals who grow up under those conditions [@elderChildrenGreatDepression2018a]. These individuals, then, would have distinctive attitudes, due to the circumstances of their upbringing, that they would then carry throughout their lives. Regarding how old they are or the cultural zeitgeist at any given moment, we could be able to tell something about these individuals' believes and behaviors by knowing when they were born. These are called *cohort effects* [@elderChildrenGreatDepression2018a; @fosseAnatomyCohortAnalysis2023].

Disentangling these different sources of cultural change is a well-known challenge [@bellImpossibilitySeparatingAge2013]. In the framework of a standard regression, examining these three sources of variation would entail perfect collinearity [@fosseAnalyzingAgeperiodcohortData2019]. This is simply because, if we know an individual's age and the current year, we also know precisely when they were born. Though this is not the place to provide a full review of the work that has been done around the estimation of age-period-cohort effects (cf. [@fosseAnalyzingAgeperiodcohortData2019]), it suffices to mention that researchers have devised several strategies o try to disaggregate these three sources of change. Nonetheless, all strategies involve a kind of compromise - e.g. turning continuous categories like year into categorical chunks to avoid collinearity [@vaiseyCulturalFragmentationAcquired2016a]. Fortunately for us, we do not need to disentangle all three of these effects. Instead, for theoretical reasons, we can focus on the contrast between a model in which only cohort replacement matters and one where within-cohort change (by whatever mechanism) is also an important mechanism of aggregate change.

### Two models of individual-level change

Broadly speaking, there are two main models that can help us think about how change unfolds at the individual level. The first one has been called the "settled-dispositions" model [@kileyMeasuringStabilityChange2020; @underwoodCohortSuccessionExplains2022]. The main argument here is that an individuals' beliefs and preferences are mainly forged during a critical period of socialization. Thus, after the learning associated with one's formative years, attitudes remain relatively stable across the lifetime. This has the further implication that individuals raised in similar socio-historical contexts will share certain attitudinal dispositions that they will then carry throughout their lives [@elderChildrenGreatDepression2018a; @gerberRationalLearningPartisan1998; @ryderCohortConceptStudy1985]. The "settled-dispositions" has a longstanding history in the social sciences, from Parsons's [-@balesFamilySocializationInteraction2014] work on the internalization of values to Mannheim's [-@mannheimProblemGenerations1970] "Problem of Generations". Perhaps the most telling evidence for the widespread influence of this model is that most of our theories of socialization acknowledge -- either explicitly or implicitly -- its validity. The very idea of socialization requires the assumption that certain dispositions, acquired during the critical period of upbringing, are particularly sticky, proving influential throughout the life-course [@guhinWhateverHappenedSocialization2021].

The second model portrays individuals as constantly open to revisiting their beliefs. This model of "active updating" is based on social theories that portray the self as continuously under construction [@grossPragmatistTheorySocial2009]. Individuals then are always open to novel information -- including biographical information gained through aging -- and, thus, highly sensitive to changes in their cultural and social contexts. This implies that cultural moments would play a much bigger in shaping individuals' attitudes. In other words, individuals will reconsider their attitudes in light of the cultural trends and/or political movements that happen to be popular at a particular historical moment. Visions of history as propelled by the zeitgeist of an epoch -- which takes hold across whole swathes of the population -- rely implicitly on the idea that individuals are attuned to the "spirit of the age", ready to change their attitudes as new information arises.

Trying to adjudicate the explanatory power of the two models has been at the center of research about large-scale change for the past two decades. The "settled-dispositions" and "active-updating" models assume quite different theories about how change occurs throughout an individual's life, and -- as a consequence -- provide widely diverging accounts of social change. In practical terms, Lizardo and Vaisey [-@vaiseyCulturalFragmentationAcquired2016a] argue that the differences between these models can be boiled down to a rather simple question: *to predict a person's attitudes at a given point are we better off knowing the current year or their date of birth?*

These two broad models have vastly different implications for the patterns we should expect to see at the collective level. If the "active updating" model were the dominant process in the social world, we would expect individuals to be highly receptive to new information and the changing characteristics of the environment. We would expect then cultural change to happen swiftly, following particular events or shocks. An unexpected economic downturn might lead the members of a group -- regardless of age -- to be more conservative in their financial choices. A series of catastrophic climate disasters might lead them to update their beliefs on human-induced climate change. In other words, extraneous changes will be reflected directly in aggregate cultural attitudes, as the population updates their attitudes in light of new information and/or circumstances. The "settled-dispositions" model paints a rather different picture. If this model were true, we should expect agents, who have passed their formative years, to be less swayed by new information. Thus, cohorts raised under the unfavorable economic circumstances or the perilous climate will develop attitudes based on these formative experiences. These beliefs, however, will only change aggregate attitudes about fiscal policy or man-made climate change, as their predecessors, raised under different circumstances, die and give way to their new ideas [@ryderCohortConceptStudy1985]. Aggregate social change in this scenario will be gradual, and the consequences of extraneous circumstances will become apparent as younger cohorts, who experience them during their critical periods, replace those than came before them.

Recent work on attitudinal change suggests that the "settled-dispositions" model explains the bulk of aggregate social change [@vaiseyCulturalFragmentationAcquired2016a; @kileyMeasuringStabilityChange2020; @underwoodCohortSuccessionExplains2022]. Lizardo and Vaisey [-@vaiseyCulturalFragmentationAcquired2016a], for instance, compare the explanatory power of both models in cross-sectional time-series data from the US. They find that most attitudes remain relatively stable within cohorts, giving weight to the idea that aggregate change is most adequately explained by cohort succession. Moreover, when looking at individual-level longitudinal data, there is evidence that points in the same direction [@kileyMeasuringStabilityChange2020; @bartelsGenerationalModelPolitical2014]. Cohorts seem to share certain dispositions, which appear to remain remarkably stable across the life-course. Thus, for both cross-sectional and individual-level longitudinal data, across many different issues, there is consistent evidence that highlights the explanatory value of the "settled-dispositions" model.

### Variation in mechanisms of change across issues

At this point, readers might be -- understandably -- conjuring up counter-examples of rapid cultural change. It is important to highlight that arguing that cultural change occurs primarily through cohort replacement does not mean stating that this is the only mechanism. Recent work by Lersch [-@lerschChangePersonalCulture2023] shows that individuals do exhibit some change in adulthood, even if this change is small relative to persistent between-person differences. Moreover, Kiley and Vaisey [-@kileyMeasuringStabilityChange2020] show that there are certain issues where we do observe considerable durable change, even among adults. In the U.S., for instance, there is evidence of within-individual updating with regards to attitudes towards homosexuality, a particularly salient issue for the past few decades in the United States. It is possible that for certain issues, that enjoy enough sustained public attention, we might observe attitudinal updating even among people with relatively settled dispositions. This evidence suggests therefore that it may be more accurate to think about updating across an individual's lifetime as probabilistic, where attitudes are more difficult -- but not impossible -- to change after the critical period of socialization. But certain issues do seem to exhibit change across the life-course, either because of particular biographical trajectories [-@lerschChangePersonalCulture2023] or perhaps because they seem to capture the public's attention [@zallerNatureOriginsMass1992]. The goal is trying to understand what it is about these issues that makes them more likely to be updated even in adulthood, and whether there are cross-cultural patterns in what these issues are.

Sensitivity is a concept that perhaps allows us to get an initial handle on this question. Campbell and Mace (this issue) define sensitive issues as those topics that are difficult to talk about. At first glance, we see that issues like gay civil rights or abortion -- which have exhibited considerable change in the last few decades -- are just such issues. In this sense, given the difficulty around discussing these topics, we should expect individuals to not change their attitudes around them much. This would mean that the swift changes we have observed -- at the aggregate level -- around this subjects should be mostly attributed to cohort replacement.

Nonetheless, as Kiley and Vaisey's [-@kileyMeasuringStabilityChange2020] work shows, these issues are also the ones where we do see evidence of individual updating. We should remember that these issues were not always considered sensitive, but rather there have been concerted efforts to bring them to the forefront of political discussions. For example, work on the history of public opinion for abortion rights suggests that the topic was not central for conservative thinking in the middle of the 20th century [@hollandTinyYouWestern2020]. This issue became sensitive through intentional efforts by the Christian right to shed light on this topic and to bring it into the core of the conservative political identity [@hollandTinyYouWestern2020].

This leaves us at a crossroads. Sensitive issues should change more slowly, and yet we have evidence of individual updating around these topics, even among those whose attitudes should be stable. The media are filled with discussions about these subjects, yet they are particularly difficult to bring up at family dinners. Sensitive issues then occupy an interesting position. For many of them, we should expect there to be a lot of inter-cohort disagreement and, thus, we should expect them to change through cohort replacement. However, given that the differences between the average attitudes of different cohorts is large, even gradual change might lead to considerable variation at the level of public opinion. Moreover, given that these issues dominate public discussion, we should also expect them to lead to more attitudinal updating among those who are less likely to revise their opinions. Therefore, so-called "sensitive issues" might change differently than other topics that are less salient and emotionally fraught. This is precisely what we seek to test in our analyses.

## Methods

### Disentangling within-cohort and between-cohort differences

The above discussion suggests that the goal is not disentangling the relative importance of age, period, and cohort effects, but rather adjudicating the relative explanatory power of the two broad models of individual-level updating. Both goals are not equivalent. If the settled-dispositions model is dominant, then we should expect most group-level cultural change to be driven by differences between cohorts, as younger individuals move away from the attitudes of those that came before them. In turn, if the "attitudinal updating" model is more explanatory, then we should see evidence of considerable changes within cohorts, as they are exposed to new information and/or as they age. The central distinction, then, is between the relative importance of *between-cohort differences* and *within-cohort change*, with issues like *period* and *age effects* already enveloped in the latter.

To clarify the distinction between patterns of large-scale change mainly driven by *between cohort differences* or *within-cohort change*, it is useful to envision two idealized models of aggregate change. First, imagine a scenario where after the critical period of socialization, cohorts settle into their dispositions and then hardly deviate from their averages. If we were able to track the data by cohort it would look like overlapping horizontal lines, with different intercepts on the y-axis. Change, at the aggregate level, would look like a gradual shift towards the averages of the younger cohorts. Figure 1 illustrates both dynamics. In this case, knowing a person's year of birth would tell us reliably their opinion. Cohort would also explain all the variation in aggregate change, given that -- in this idealized scenario -- all change occurs through cohort replacement and, therefore, large-scale variation is exclusively the result of *between-cohort differences*.

```{r}
set.seed(2607)
cohort_1920 <- tibble(
  year=(1920+18):2000,
  opinion=rnorm(length(year), 0.1, sd = 0.1)
)
cohort_1940 <- tibble(
  year=(1940+18):2020,
  opinion=rnorm(length(year), 0.3, sd = 0.1)
)
cohort_1960 <- tibble(
  year=(1960+18):2020,
  opinion=rnorm(length(year), 0.5, sd = 0.1)
)

all_cohorts <- rbind(cohort_1920, cohort_1940, cohort_1960)

cohort_plot <- all_cohorts %>% 
  mutate(cohort = rep(c("1920", 
                        "1940", 
                        "1960"), 
                      times = c(63, 63, 43))) %>% 
  ggplot(aes(x = year, 
             y = opinion, 
             color = cohort, 
             group = cohort)) +
  geom_point(pch = 23) +
  geom_segment(aes(xend = 2020, 
                   x = 1978, 
                   y = 0.5, 
                   yend = 0.5), 
               col = "blue") +
  geom_segment(aes(xend = 2020, 
                   x = 1958, 
                   y = 0.3, 
                   yend = 0.3), 
               col = "forestgreen") +
  geom_segment(aes(xend = 2000, 
                   x = 1938, 
                   y = 0.1, 
                   yend = 0.1), 
               col = "red") +
  labs(y = "Opinion", 
       x = "Year", 
       color = "Cohort", 
       title = "Opinions by Cohort")


aggregate_change <- all_cohorts %>% 
  mutate(cohort = rep(c("1920", 
                        "1940", 
                        "1960"), 
                      times = c(63, 63, 43))) %>% 
  group_by(year) %>% 
  summarise(avg = mean(opinion)) %>% 
  ggplot(
    aes(x = year, 
        y = avg)
  ) +
  geom_point(pch = 23) +
  geom_line(color = "firebrick") +
  labs(
    y = "Avg. Opinion", 
    x = "Year", 
    title = "Aggregate Change"
  )

cohort_plot + aggregate_change
```

Now, imagine another scenario where cohorts do update their attitudes around an issue, either because as individuals get older they tend to change their beliefs or because an issue has been particularly salient in public discussions. In other words, we would assume that there are, in addition to initial *between-cohort differences*, *within-cohort changes* in this example, which can be either *period* or *age* effects. In this stylized example, we can imagine an issue -- like attitudes towards homosexuality -- that has become increasingly important in the public sphere since the middle of the 20th century and where individuals seem to have updated their beliefs. Figure 2 shows this second example. We notice here that we see *within cohort changes*, due to common trends experienced by all members of the group. This, in turn, translates into much steeper cultural change at the aggregate level. Cultural change here is not only due to the overall differences between cohorts - and their replacement - but also due to variation in opinions, in the same direction, within cohorts.

```{r}
coh_plot_period <- all_cohorts %>% 
  mutate(cohort = rep(c("1920", 
                        "1940", 
                        "1960"), 
                      times = c(63, 63, 43)), 
         opinion = if_else(
           year <1960, 
           opinion, 
           opinion + (year-1938)*0.01
         )) %>% 
  ggplot(aes(x = year, 
             y = opinion, 
             color = cohort, 
             group = cohort)) +
  geom_point(pch = 23) +
  geom_smooth(method = "lm", 
              se=F) +
  labs(y = "Opinion", 
       x = "Year", 
       color = "Cohort", 
       title = "Opinions by Cohort") 
  

aggregate_change_period <- all_cohorts %>% 
  mutate(cohort = rep(c("1920", 
                        "1940", 
                        "1960"), 
                      times = c(63, 63, 43)), 
         opinion = if_else(
           year <1960, 
           opinion, 
           opinion + (year-1938)*0.01
         )) %>% 
group_by(year) %>% 
  summarise(avg = mean(opinion)) %>% 
  ggplot(
    aes(x = year, 
        y = avg)
  ) +
  geom_point(pch = 23) +
  geom_line(color = "firebrick") +
  labs(
    y = "Avg. Opinion", 
    x = "Year", 
    title = "Aggregate Change"
  )

coh_plot_period + aggregate_change_period
```

A more extreme variant of this example would be one where all cohorts start from the same average opinion -- regardless of the current year -- and endure the same *within-cohort changes*. In other words, we can imagine a scenario where there are no initial *between-cohort differences* and all age-groups follow the same trends in opinion change. Figure 3 illustrates such a case:

```{r}


set.seed(2607)
cohort_1920 <- tibble(
  year=(1920+18):2000,
  opinion=rnorm(length(year), 0.1, sd = 0.05)
)
cohort_1940 <- tibble(
  year=(1940+18):2020,
  opinion=rnorm(length(year), 0.1, sd = 0.05)
)
cohort_1960 <- tibble(
  year=(1960+18):2020,
  opinion=rnorm(length(year), 0.1, sd = 0.05)
)

all_cohorts_same <- rbind(cohort_1920, cohort_1940, cohort_1960)

coh_plot_period <- all_cohorts_same %>% 
  mutate(cohort = rep(c("1920", 
                        "1940", 
                        "1960"), 
                      times = c(63, 63, 43))) %>% 
  group_by(cohort) %>% 
  mutate(year0 = year-min(year)+1, 
         opinion = opinion + year0 * 0.01) %>% 
  ungroup()  %>% 
  ggplot(aes(x = year, 
            y = opinion, 
            color = cohort, 
            group = cohort)) +
  geom_point(pch = 23) +
  geom_smooth(method = "lm", 
              se=F) +
  labs(y = "Opinion", 
       x = "Year", 
       color = "Cohort", 
       title = "Opinions by Cohort") 

aggregate_change_period <- all_cohorts_same %>% 
  mutate(cohort = rep(c("1920", 
                        "1940", 
                        "1960"), 
                      times = c(63, 63, 43))) %>% 
  group_by(cohort) %>% 
  mutate(year0 = year-min(year)+1, 
         opinion = opinion + year0 * 0.01) %>% 
  ungroup() %>% 
  group_by(year) %>% 
  summarise(avg = mean(opinion)) %>% 
  ggplot(
    aes(x = year, 
        y = avg)
  ) +
  geom_point(pch = 23) +
  geom_line(color = "firebrick") +
  labs(
    y = "Avg. Opinion", 
    x = "Year", 
    title = "Aggregate Change"
  )

coh_plot_period + aggregate_change_period
```

Based on these idealized models, we propose a simple test that can help us differentiate the relative contribution of *within-cohort change* and *between-cohort differences* for understanding aggregate cultural change. We can fit two models to the same data. One would be a linear regression where the outcome variable is regressed over the cohort of each respondent:

$$ y_i \sim N(\mu, \sigma^2) $$ $$ \mu = \alpha + \beta_{\text{cohort}[i]} $$

The second model would be similar but it would also include an interaction effect between cohort and year, to include the possibility that there have been period effects, and these effects influence each cohort in a different way:

$$ y_i \sim N(\mu, \sigma^2) $$

$$ \mu = \alpha + \beta_{\text{cohort}[i]} + \phi \times \text{year}_i + \zeta_{\text{cohort[i]}} \times \text{year}_i $$

```{r}
# Functions ----
getmod_lm_b2 <- function(df) {
  lm(y ~ cohort10t,
     data = df)
}

getmod_lm_bw2 <- function(df) {
  lm(y ~ ns(year0, df = 2) * cohort10t,
     data = df)
}

# get R2
get_r2 <- function(mod) {
  r2 <- r2(mod) %>% .[[1]] %>% as.numeric()
  return(r2)
}


all_cohorts_replace <- all_cohorts %>% 
  mutate(cohort = rep(c("1920", 
                        "1940", 
                        "1960"), 
                      times = c(63, 63, 43)), 
         year0 = year-1938)

mod1 <- lm(opinion ~ cohort, all_cohorts_replace)
mod2 <- lm(opinion ~ ns(year0, df = 2) * cohort, 
           data = all_cohorts_replace)

first_mod_r2 <- round(get_r2(mod1)/get_r2(mod2), 2) 


all_cohorts_period <- all_cohorts %>%
  mutate(
    cohort = rep(c("1920",
                   "1940",
                   "1960"),
                 times = c(63, 63, 43)),
    year0 = year - 1938,
    opinion = if_else(year < 1960,
                      opinion,
                      opinion + (year - 1938) * 0.01)
  )

mod3 <- lm(opinion ~ cohort, all_cohorts_period)
mod4 <- lm(opinion ~ ns(year0, df = 2) * cohort, 
           data = all_cohorts_period)

second_mod_r2 <- round(get_r2(mod3)/get_r2(mod4), 2) 

all_cohorts_nodiff <- all_cohorts_same %>%
  mutate(cohort = rep(c("1920", 
                        "1940", 
                        "1960"), 
                      times = c(63, 63, 43))) %>% 
  group_by(cohort) %>% 
  mutate(year0 = year-min(year)+1, 
         opinion = opinion + year0 * 0.01) %>% 
  ungroup()

mod5 <- lm(opinion ~ cohort, all_cohorts_nodiff)
mod6 <- lm(opinion ~ ns(year0, df = 2) * cohort, 
           data = all_cohorts_nodiff)

third_mod_r2 <- round(get_r2(mod5)/get_r2(mod6), 2) 
```

We then compare the variance explained by each model. Specifically, we divide the variance explained by the first model over that accounted for by the second one.

$$ \tau = \frac{V(BD)}{V(BD+WC)} $$

$\tau$ then could be interpreted as the proportion of variance explained that is preserved when only *between-cohort differences* matter - i.e. when we remove the effect of time from the model. As the number gets closer to one, then, this would be an indication that change in a given variable is mostly accounted for by cohort replacement as opposed to period effects. For example, in our rather simple scenarios, $\tau$ for the first case would be `r first_mod_r2` and in the second case would be `r second_mod_r2`. For the third -- admittedly extreme -- case, $\tau$ would be `r third_mod_r2`. Almost all variance explained is preserved in the first case when we take the effect of year out of the model. In the second case, we do seem to lose information, as we expected. In the third case, almost *none* of the variance explained is preserved when we do not consider *within-cohort changes*, as these explain almost all the change in aggregate opinion in this case. This simple test, then, is a useful indicator to begin to differentiate the mechanisms that underpin the large-scale opinion change.

At this point it is important to discuss an important assumption in this approach. The second model we fit assumes that *within-cohort change* is linear. This means that we are unable to capture within-cohort fluctuations that might be caused by temporary shocks -- e.g. a temporary increase in support of defense spending after a terrorist attack. While we acknowledge that these fluctuations are an important component of *within-cohort change*, they cannot account for monotonic aggregate changes over time. When social scientists discuss cultural change, they often mean *directional* change. In other words, we tend to be interested in variation that follows a trend, like the secularization of group or the expansion/erosion of civil liberties. Given that we are interested in how average opinions, within cohorts, have changed in a single direction across time, the linear assumption is warranted. However, this does prevent us from drawing any conclusions about any *non-linear* changes that might happen within cohorts, which can certainly be important for understanding opinion change.

### Data & Analysis

To understand mechanisms of change, across different contexts and issues, we use the World Values Survey (WVS) [@inglehartWorldValuesSurveys2000]. The WVS is a large-scale effort to collect comparable attitudinal data across multiple countries, which started in 1981. For each country and each wave, the WVS collects high-quality, nationally representative samples, and covers a wide range of questions from views on gender equality to socio-economic indices. The survey, however, is not longitudinal, which means that we are unable to track any within-individual changes across time. However, it allows us to examine trends in aggregate opinion across time for different countries.

Our method will be most insightful when we have information about aggregate information in each country across a considerable span of time. This is a challenge because not all countries feature in every wave, and not all questions were asked in the times when we do have samples. We decide to select countries based on completeness: those for which we have the most measures. This leads us to eight countries: Argentina (ARG), Australia (AUS), Canada (CAN), Japan (JPN), Mexico (MEX), South Africa (ZAF), Sweden (SWE), and the USA. This is not comprehensive or particularly diverse sample of countries. We are missing some of the world's most populous countries -- India and China -- and we do not have a majority-Muslim country. However, given that these mechanisms of social change have yet to be tested across different societies, an initial comparison -- albeit limited -- is valuable. We also select the variables for our analysis based on relevance and completeness. In terms of the former, we choose variables that reflect cultural attitudes that could plausibly change over time. This includes a wide range of items, from opinions about child-bearing to attitudes about the acceptability of euthanasia. We also select the variables based on whether they have been asked in all the waves for the countries selected. After implementing both criteria we are left with 56 variables that cover a wide variety of issues, some everyday and some highly sensitive. The full list of items, alongside their respective questions and the abbreviations we use below, is available in the supplementary materials.

Given that we are interested in how sensitive or mundane these questions are, we fielded a multi-country survey to get at this issue. Though operationalizing sensitivity is difficult, we find the definition given in this special issue a useful starting point. As mentioned above, sensitivity here is defined as those topics that are difficult to talk about. We took this definition and asked respondents to tell us how easy or difficult it would be to discuss a given question, as worded in the WVS. Importantly, we are not interested in the respondents' own opinions on a given issue, but rather on how difficult they think it would be for the *majority of their compatriots* to talk about that question. Thus, we asked them: "how difficult would it be for the majority of people from you country to discuss the following question". We then provided them with a scale from one to ten, where the lower end was labelled as "not difficult at all" and the higher end was labeled as "extremely difficult". Each participant rated all the questions associated with the variables that we consider in this study. This allows us to approach a defensible measure of how sensitive each issue is in each of the eight countries we are focusing on. 

To field the surveys, we used the online platforms Prolific and CloudResearch. Both offer a high-quality pool of respondents and, importantly, they maintain samples across different countries. Between both platforms, we were able to reach to respondents from the eight countries that constitute our WVS sample. We translated all the questions to the main languages spoken in each country, and we gave participants the opportunity to choose their preferred language. Initially, our sample consisted of 808 individuals and, after excluding participants that had missed more than two attention checks, we had total sample of 802 respondents. Table 1 breaks down how this sample is distributed across the countries: 

```{r}

usa_d <- read_csv("Data/usa_data_full.csv") %>% mutate(country = "USA")
# Keep only the real answers (no headers, pretests or rejections)
usa_d <- usa_d[-c(1:5, 16:17, 19) ,]

swe_d <- read_csv("Data/swe_data_full.csv") %>% mutate(country = "SWE")
swe_d <- swe_d[-c(1:2),]

zaf_d <- read_csv("Data/zaf_data_full.csv") %>% mutate(country = "ZAF")
zaf_d <- zaf_d[-c(1:2),]

mex_d <- read_csv("Data/mex_data_full.csv") %>% mutate(country = "MEX")
mex_d <- mex_d[-c(1:2),]

jpn_d <- read_csv("Data/jpn_data_full.csv") %>% mutate(country = "JPN")
jpn_d <- jpn_d[-c(1:2),]

can_d <- read_csv("Data/can_data_full.csv") %>% mutate(country = "CAN")
can_d <- can_d[-c(1:2),]

aus_d <- read_csv("Data/aus_data_full.csv") %>% mutate(country = "AUS")
aus_d <- aus_d[-c(1:2),]

arg_d <- read_csv("Data/arg_data_full.csv") %>% mutate(country = "ARG")
arg_d <- arg_d[-c(1:8,60),]


usa_ds <- usa_d %>% 
  select(23:77,81)

swe_ds <- swe_d %>% 
  select(23:77,81)

zaf_ds <- zaf_d %>% 
  select(23:77,81)

mex_ds <- mex_d %>% 
  select(23:77,81)

jpn_ds <- jpn_d %>% 
  select(23:77,81)

can_ds <- can_d %>% 
  select(23:77,81)

aus_ds <- aus_d %>% 
  select(23:77,81)

arg_ds <- arg_d %>% 
  select(22:76,81)

d_sens_complete <- rbind(usa_ds,
                         swe_ds,
                         zaf_ds,
                         mex_ds,
                         jpn_ds,
                         can_ds,
                         aus_ds, 
                         arg_ds)

d_sens_complete <- d_sens_complete %>% 
  mutate(
    across( 
      .cols =1:55, 
      as.numeric
    )
  )

# If attention check succesful then 1, else 0
# Count attention checks
d_sens_complete <- d_sens_complete %>% 
  mutate(ac1 = if_else(att1_1 == 2, 1, 0), 
         ac2 = if_else(att2_1 == 5, 1, 0), 
         ac3 = if_else(att3_1 == 1, 1, 0), 
         ac = ac1+ac2+ac3)
rm(usa_d,
   swe_d,
   zaf_d,
   mex_d,
   jpn_d,
   can_d,
   aus_d)

rm(usa_ds,
   swe_ds,
   zaf_ds,
   mex_ds,
   jpn_ds,
   can_ds,
   aus_ds)

# Keep only respondents with 2 or more attention checks
d_sens_complete <- d_sens_complete %>% 
  filter(ac >=2)

d_sens_long <- d_sens_complete %>% 
  select(country, everything()) %>% 
  mutate(
    across( 
      .cols = 2:56, 
      as.numeric
    )
  ) %>% 
  pivot_longer(
    2:56, 
    names_to = "question", 
    values_to = "difficulty"
  ) %>% 
  mutate(
    question = str_remove(question, "_1$"), 
    variable = case_when( 
      question == "imp_family" ~ "important_family", 
      question == "imp_friends" ~ "important_friends", 
      question == "imp_leisure" ~ "important_leisure", 
      question == "imp_pol" ~ "important_politics", 
      question == "imp_wrk" ~ "important_work", 
      question == "imp_rel" ~ "important_religion", 
      question == "child_qualities" ~ "child_independence", 
      question == "neigh_race" ~ "neigh_diff_race", 
      question == "neigh_drink" ~ "neigh_drink", 
      question == "neigh_imm" ~ "neigh_imm", 
      question == "neigh_aids" ~ "neigh_aids", 
      question == "neigh_drug" ~ "neigh_drugs", 
      question == "neigh_gay" ~ "neigh_gay", 
      question == "trust" ~ "trust_people", 
      question == "life_satisf" ~ "life_satisf", 
      question == "choice_control" ~ "choice_control", 
      question == "scarce_jobs_women" ~ "jobs_men_over_women", 
      question == "scarce_jobs_imm" ~ "jobs_national_over_foreign", 
      question == "housewife_fulfill" ~ "housewife_fulfilling", 
      question == "left_right" ~ "politics_scale", 
      question == "equal_incomes" ~  "income_eq", 
      question == "private_public_busi" ~ "pvt_state_owned", 
      question == "gvt_responsibility" ~ "gvt_responsibility", 
      question == "comp_good_bad" ~ "competition_good_evil", 
      question == "conf_church" ~ "confidence_churches", 
      question == "conf_armed" ~ "confidence_armed_forces", 
      question == "conf_press" ~ "confidence_press", 
      question == "conf_unions" ~ "confidence_unions", 
      question == "conf_police" ~ "confidence_police", 
      question == "conf_parliament" ~ "confidence_parliament", 
      question == "conf_civil" ~ "confidence_civil", 
      question == "conf_tv" ~ "confidence_tv", 
      question == "conf_gvt" ~ "confidence_government", 
      question == "conf_pol_prt" ~ "confidence_political_parties", 
      question == "conf_companies" ~ "confidence_major_companies", 
      question == "conf_courts" ~ "confidence_justice_courts", 
      question == "rel_services" ~ "attend_relig", 
      question == "religiosity" ~ "religious_person", 
      question == "belief_god" ~ "believe_god", 
      question == "belief_hell" ~ "believe_hell", 
      question == "imp_god" ~ "important_god", 
      question == "just_gvt_benefits" ~  "just_gvt_benefits", 
      question == "just_public_trans" ~ "just_fare_public_trans", 
      question == "just_cheat_tax" ~ "just_cheat_taxes", 
      question == "just_bribe" ~ "just_bribe", 
      question == "just_homo" ~ "just_homosexuality", 
      question == "just_prst" ~ "just_prostitution", 
      question == "just_abort" ~ "just_abortion", 
      question == "just_divorce" ~ "just_divorce", 
      question == "just_euth" ~ "just_euthanasia", 
      question == "just_suicide" ~ "just_suicide", 
      question == "proud" ~ "proud_nationality", 
      TRUE ~ question),
  )


d_sens_complete %>% 
  group_by(country) %>% 
  summarise(`Number of Respondents` = n()) %>% 
  knitr::kable(
    format = "latex",
    align = "c",
    booktabs = TRUE,
    longtable = TRUE,
    linesep = "",
    ) %>%
  kableExtra::kable_styling(
      position = "left",
      latex_options = c("striped", "repeat_header"),
      stripe_color = "gray!15"
    )
  
```

Now, we do not claim that this sample is representative of the population of any of those countries. However, we believe that these data provide a principled measure of how sensitive certain issues are perceived in each country. The fact that we prompted participants to think about their second-order beliefs -- to think about what most of their compatriots think -- helps in trying to bypass individual idiosyncrasies and to get a adequate measure of group-level perceptions of sensitivity. This is reflected I the fact that the overall trends in our data display plausible results. Figure X, for example, shows the median score for the five issues that respondents, in each country, rated as the most difficult to discuss[^We provide full descriptive results of the survey data in the supplementary materials]: 

```{r}
p_usa <- d_sens_long %>% 
  filter(country == "USA") %>% 
  group_by(variable) %>% 
  summarise(avg = median(difficulty)) %>% 
  ungroup() %>% 
  arrange(desc(avg)) %>% 
  slice(1:5) %>% 
  ggplot(
    aes(x = fct_reorder(variable, avg), 
        y = avg)
  ) +
  geom_point(pch=23) +
  coord_flip() +
  labs(title = "USA", 
       y = "Median Difficulty", 
       x = "") +
  theme(text = element_text(size = 6)) 

p_swe <- d_sens_long %>% 
  filter(country == "SWE") %>% 
  group_by(variable) %>% 
  summarise(avg = median(difficulty)) %>% 
  ungroup() %>% 
  arrange(desc(avg)) %>% 
  slice(1:5) %>% 
  ggplot(
    aes(x = fct_reorder(variable, avg), 
        y = avg)
  ) +
  geom_point(pch=23) +
  coord_flip() +
  labs(title = "SWE", 
       y = "Median Difficulty", 
       x = "") +
  theme(text = element_text(size = 6)) 

p_zaf <- d_sens_long %>% 
  filter(country == "ZAF") %>% 
  group_by(variable) %>% 
  summarise(avg = median(difficulty)) %>% 
  ungroup() %>% 
  arrange(desc(avg)) %>% 
  slice(1:5) %>% 
  ggplot(
    aes(x = fct_reorder(variable, avg), 
        y = avg)
  ) +
  geom_point(pch=23) +
  coord_flip() +
  labs(title = "ZAF", 
       y = "Median Difficulty", 
       x = "") +
  theme(text = element_text(size = 6)) 

p_mex <- d_sens_long %>% 
  filter(country == "MEX") %>% 
  group_by(variable) %>% 
  summarise(avg = median(difficulty)) %>% 
  ungroup() %>% 
  arrange(desc(avg)) %>% 
  slice(1:5) %>% 
  ggplot(
    aes(x = fct_reorder(variable, avg), 
        y = avg) 
  ) +
  geom_point(pch=23) +
  coord_flip() +
  labs(title = "MEX", 
       y = "Median Difficulty", 
       x = "") +
  theme(text = element_text(size = 6)) 

p_jpn <- d_sens_long %>% 
  filter(country == "JPN") %>% 
  group_by(variable) %>% 
  summarise(avg = median(difficulty)) %>% 
  ungroup() %>% 
  arrange(desc(avg)) %>% 
  slice(1:5) %>% 
  ggplot(
    aes(x = fct_reorder(variable, avg), 
        y = avg)
  ) +
  geom_point(pch=23) +
  coord_flip() +
  labs(title = "JPN", 
       y = "Median Difficulty", 
       x = "") +
  theme(text = element_text(size = 6)) 

p_can <- d_sens_long %>% 
  filter(country == "CAN") %>% 
  group_by(variable) %>% 
  summarise(avg = median(difficulty)) %>% 
  ungroup() %>% 
  arrange(desc(avg)) %>% 
  slice(1:5) %>% 
  ggplot(
    aes(x = fct_reorder(variable, avg), 
        y = avg)
  ) +
  geom_point(pch=23) +
  coord_flip() +
  labs(title = "CAN", 
       y = "Median Difficulty", 
       x = "") +
  theme(text = element_text(size = 6)) 

p_arg <- d_sens_long %>% 
  filter(country == "ARG") %>% 
  group_by(variable) %>% 
  summarise(avg = median(difficulty)) %>% 
  ungroup() %>% 
  arrange(desc(avg)) %>% 
  slice(1:5) %>% 
  ggplot(
    aes(x = fct_reorder(variable, avg), 
        y = avg)
  ) +
  geom_point(pch=23) +
  coord_flip() +
  labs(title = "ARG", 
       y = "Median Difficulty", 
       x = "") +
  theme(text = element_text(size = 6)) 

p_aus <- d_sens_long %>% 
  filter(country == "AUS" & 
           !str_detect(variable,"att")) %>% 
  group_by(variable) %>% 
  summarise(avg = median(difficulty)) %>% 
  ungroup() %>% 
  arrange(desc(avg)) %>% 
  slice(1:5) %>% 
  ggplot(
    aes(x = fct_reorder(variable, avg), 
        y = avg)
  ) +
  geom_point(pch=23) +
  coord_flip() +
  labs(title = "AUS", 
       y = "Median Difficulty", 
       x = "") +
  theme(text = element_text(size = 6)) 

p_usa + p_swe + p_zaf + p_mex + p_jpn + p_can + p_arg + p_aus

```

Unsurprisingly, the questions about the justifiability of abortion, euthanasia, and suicide feature prominently in most countries, showing cross cultural similarities in perceptions of sensitivity. However, we also pick up on some context-specific patterns: while issues of corruption and government intervention are perceived as difficult to discuss in Argentina, question about race and immigration are seen as particularly sensitive in Sweden. This resonates with both the recent past of these countries and their current circumstances. The full descriptive summary of the data is provided in the supplementary materials, but overall the results seem to capture intuitive general trends while allowing for context-specific variation. While capturing the notion of sensitivity is challenging, we believe our data is an adequate operationalization, at least for the purposes of the present study. 

We begin our analyses by examining the relationship between $\tau$ and the total amount of change each variable exhibits. For each variable, we calculate $\tau$, as defined above, and then plot it against how much it has changed across the decades of observation. We then fit two regression models to explore whether the perceived sensitivity of an issue is related to how much they change and to the mechanism that most likely underpins that variation. In both regressions, the key independent variable of interest is the median sensitivity for each country and each variable. ^[We use the median because it is less sensitive to extreme values, but our results show similar patterns when we use the average sensitivity for each variable]. In turn, the dependent variables of interest are respectively: the absolute change exhibited by each variable in each country and the calculated $\tau$. In both models, we allow for varying intercepts and slopes across countries to account for cultural variation -- and similarities -- between the different contexts. 

```{r}
# Data importation & cleaning ----
load("Data/wvs_timeseries.rdata")
d <- data1
rm(data1)

# Read in the data 
d_short <- d %>% 
  select(S002VS, 
         COUNTRY_ALPHA,
         S020,
         S003,
         X001,
         X003, 
         X007, 
         A001:A006,
         A029:A042,
         A124_02, 
         A124_03, 
         A124_06:A124_09,
         A165,
         A170,
         A173,
         C001,
         C002,
         D057,
         E033,
         E035:E037,
         E039,
         E069_01:E069_02,
         E069_04:E069_08,
         E069_10:E069_13,
         E069_17,
         F028,
         F034,
         F050,
         F053,
         F063,
         F114A,
         F115:F123,
         G006) %>% 
  rename(wave = S002VS, 
         country = COUNTRY_ALPHA,
         country_code = S003, 
         year_survey = S020,
         sex = X001,
         age = X003, 
         
         # Importance battery: 1 means very important, 4 means not at all important
         important_family = A001,
         important_friends = A002, 
         important_leisure = A003, 
         important_poltics = A004,
         important_work = A005, 
         important_religion = A006, 
         
         # Important qualities in children: 1 means mentioned, 0 means not mentioned
         child_independence = A029, 
         child_hard_work = A030, 
         child_feeling_responsibility = A032, 
         child_imagination = A034, 
         child_tolerance = A035, 
         child_thrift = A038, 
         child_determination = A039, 
         child_religion = A040, 
         child_unselfish = A041,
         child_obedience = A042,
         
         # People you would not like to have as neighbours: 1 mentioned, 0 means not mentioned
         neigh_diff_race = A124_02, 
         neigh_drink = A124_03, 
         neigh_imm = A124_06, 
         neigh_aids = A124_07, 
         neigh_drugs = A124_08, 
         neigh_gay = A124_09,
         
         # Most people can be trusted: (1) Most people, (2) you can never be too careful
         trust_people = A165,
         
         # (10) means more satisfied
         life_satisf = A170, 
         
         # Control over life's choices: (10) feels most in control. 
         choice_control = A173, 
         
         # Men should be given jobs over women: (1) Agree, (2) Disagree, (3) Neither A nor D
         jobs_men_over_women = C001, 
         
         # Nationals should be given jobs over foreigners: (1) Agree, (2) Disagree, (3) Neither A nor D
         jobs_national_over_foreign = C002, 
         
         # Likert: (1) Strongly Agree -- (4) Strongly Disagree
         housewife_fulfilling = D057,
         
         # Left/Right placement: (1) Left -- (10) Right
         politics_scale = E033,
         
         # Importance of income equality 
         income_eq = E035, 
         
         # Business should be public/private owned: (1) Private -- (10) Public
         pvt_state_owned = E036, 
         
         # Government should take responsibility vs individual responsibility: (1) Gvt -- (10) Individual
         gvt_responsibility = E037,
         
         # Competition good or harmful: (1) Good --- (10) harmful
         competition_good_evil = E039, 
         
         # Confidence battery: (1) A great deal; (2) Quite a lot; (3) Not very much; (4) None at all
         confidence_churches = E069_01, 
         confidence_armed_forces = E069_02,
         confidence_press = E069_04, 
         confidence_unions = E069_05, 
         confidence_police = E069_06, 
         confidence_parliament = E069_07,
         confidence_civil = E069_08,
         confidence_television = E069_10,
         confidence_governement = E069_11,
         confidence_political_party = E069_12,
         confidence_major_companies = E069_13,
         confidence_justice_courts = E069_17,
         
         # How often attends religious services?
         # (1) More than once a week -- (8) Never
         attend_relig = F028, 
         
         # (1) Religious; (2) not religious; (3) atheist
         religious_person = F034,
         
         # Believes (1), does not believe (0)
         believe_god = F050, 
         believe_hell = F053, 
         
         # (1) Not at all important -- (10) very important
         important_god = F063,
         just_gvt_benefits = F114A,
         just_fare_public_trans = F115,
         just_cheat_taxes = F116,
         just_bribe = F117,
         just_homosexuality = F118, 
         just_prostitution = F119, 
         just_abortion = F120, 
         just_divorce = F121, 
         just_euthanasia = F122, 
         just_suicide = F123, 
         proud_nationality = G006, 
         marital_status = X007) 



# Zap labels 
d_short <- haven::zap_labels(d_short)

d_waves <- d_short %>% 
  pivot_longer(important_family:proud_nationality,
               names_to = "variable",
               values_to = "y") %>%
  mutate(age01 = (age - 25)/(64 - 25),
         birthyear = year_survey - age,
         year0 = year_survey - 1981,
         cohort5 = floor(birthyear/5) * 5,
         cohort10 = floor(birthyear/10) * 10) %>%
  drop_na(c(y, birthyear)) %>% 
  group_by(country, variable) %>% 
  mutate(start = min(year_survey),
         end = max(year_survey),
         span = end - start,
         waves = n_distinct(wave),
         obs = n()) %>% 
  ungroup()

# Select sensitivity measures for country-year 
sens_df <- d_waves %>% 
  filter(waves >= 4 & span >= 30 & age > 24) %>% 
  group_by(country,
           variable) %>% 
  mutate(num_people = n()) %>% 
  ungroup() %>% 
  group_by(country, 
           variable,
           y) %>% 
  mutate(num_answer = n(), 
         percentage_answer = num_answer/num_people) %>% 
  slice(1) %>% 
  ungroup() %>% 
  filter(y == -2) %>% 
  select(country, variable, percentage_answer)

# Previous analyses 

d_short_clean <- d_short %>% 
  mutate(
    across( 
      .cols = 5:68, 
      ~ ifelse(
        . < 0, 
        NA_real_, 
        .
      ))
  )

# Create the variables for manipulation

d_waves_clean <- d_short_clean %>% 
  pivot_longer(important_family:proud_nationality,
               names_to = "variable",
               values_to = "y") %>%
  mutate(age01 = (age - 25)/(64 - 25),
         birthyear = year_survey - age,
         year0 = year_survey - 1981,
         cohort5 = floor(birthyear/5) * 5,
         cohort10 = floor(birthyear/10) * 10) %>%
  drop_na(c(y, birthyear)) %>% 
  group_by(country, variable) %>% 
  mutate(start = min(year_survey),
         end = max(year_survey),
         span = end - start,
         waves = n_distinct(wave),
         obs = n()) %>% 
  ungroup()

# Filter dataset appropriately 
d30 <- d_waves_clean %>% 
  filter(waves >= 4 & span >= 30 & age > 24)

d30 <- d30 %>% 
  mutate(cohort10t = case_when(
    cohort10 < 1920 ~ 1920,
    cohort10 > 1980 ~ 1980,
    TRUE ~ cohort10
  ))

# Functions 
getmod_lm_b2 <- function(df) {
  lm(y ~ cohort10t,
     data = df)
}

getmod_lm_bw2 <- function(df) {
  lm(y ~ ns(year0, df = 1) * cohort10t,        # spline (or linear; check current setting)
     data = df)                                # df=2 means one bend; df=1 means line
}

# get R2
get_r2 <- function(mod) {
  r2 <- r2(mod) %>% .[[1]] %>% as.numeric()
  return(r2)
}

# Analysis

# Nest dataset
d30_nest <- d30 %>% 
  group_by(country, variable) %>% 
  nest()

# Carry out analyses
results <- d30_nest %>% 
  mutate(mb = map(data, getmod_lm_b2),
         mbw = map(data, getmod_lm_bw2),
         rb = map_dbl(mb, get_r2),
         rbw = map_dbl(mbw, get_r2),
         pbetween = rb/rbw) %>% 
  select(country, variable, rb, rbw, pbetween)

# CHANGING X-AXIS TO ABSOLUTE CHANGE 

# Find variables with meaningful changes from first to last wave
d_change <- d_waves_clean %>%
  filter(waves >= 4 & span >= 30 & age > 24) %>% 
  mutate(first_obs = min(year0),
         last_obs = max(year0),
         .by = c(country, variable)) %>% 
  filter(year0 == first_obs | year0 == last_obs) %>% 
  mutate(post = if_else(year0 == last_obs, 1L, 0L)) %>%
  mutate(ymean = mean(y),
         .by = c(country, variable, post)) %>% 
  mutate(ysd = sd(y),
         .by = c(country, variable)) %>%
  group_by(country, variable, post) %>%
  select(ymean, ysd) %>% 
  slice_head(n = 1) %>%
  ungroup() %>% 
  pivot_wider(names_from = post,
              values_from = ymean,
              names_prefix = "mean") %>% 
  mutate(change = (mean1 - mean0) / ysd,
         abschange = abs(change))

# attach to results
results <- left_join(results,
                     select(d_change, country, variable, abschange))


```

## Results

We begin by calculating $\tau$ for all variables across each country. Figure X displays the relationship between $\tau$ on the y-axis and absolute change of the variable across the recorded time span on the x-axis:

```{r}
# Results 
ggplot(results,
       aes(x = abschange,
           y = pbetween,
           label = variable, 
           color = variable)) +
  geom_point(alpha = 0.3) + 
  geom_text_repel(
    data = results %>% 
      filter(abschange >= .8), 
    size = 2.5, 
    box.padding = 0.75
  ) +
  facet_wrap(~country) +
  theme(legend.position = "none") +
  labs(
    title = "Relationship between Tau and Change", 
    y = "Tau", 
    x = "Standardized change"
  )
```

On the y-axis, we have our measure $\tau$, which can be interpreted variance explained that is preserved when *within-cohort changes* are removed from the model. On the x-axis, we have the absolute change - in standard deviations - between the first and last wave. Therefore, in the upper right quadrant of each plot, we should see variables that have changed a lot and whose change is explained mostly by cohort replacement. On the lower right quadrant, we should see variables that have also exhibited a lot of change but whose variations are mostly accounted for by *within-cohort change*.

The first striking result is that most variables do not display considerable change - i.e. most variables hover around the left-hand side of the x-axis. This is most evident in countries like Mexico and South Africa. In the plot, we label the variables that have displayed a directional change higher than 0.8 standard deviations. Another important pattern that emerges is that, consistent with previous work on cultural change [@kileyMeasuringStabilityChange2020], the variable that seems to display consistently large change across countries is the justifiability of homosexuality. Moreover, notice that this variable tends to be on upper-right quadrant, which suggests that this change tends to be driven mostly by *between-cohort differences*, rather than *within-cohort change*. In fact, at first glance, we see that this seems to be a common patterns for sensitive issues such as the justifiability of abortion, euthanasia, and divorce. When they do exhibit considerable change, most of the variation is explained by cohort differences. The variables that exhibit change through *within-cohort change* are mostly related to confidence in institutions and childrearing. Our results do seem to capture certain *period effects* that we would expect to show up given the time when the surveys were administered. For instance, changes in confidence in justice courts in Argentina coincide with the famous trials of the military dictatorship and the variation in confidence in the armed forces in Japan runs parallel with a restructuring of that institution. Thus, there seems to be a pattern in how change in different variables is underpinned mostly by *within-cohort changes* and *between-cohort differences*, and it seems to be organized around sensitivity. We test this idea properly below.

In our background section, we suggest that sensitive issues occupy an interesting position: they enjoy sustained public attention and individuals' attitudes seem particularly entrenched. We then hypothesize that these issues might display different patterns of cultural change, but our hypothesis is non-directional. Next, we test two ideas. First, we examine whether the sensitivity of an issue is predictive of how much change it has undergone. Second, we analyze whether sensitive issues seem to change more through *within-cohort changes* or *between-cohort differences* -- i.e. whether an issue's sensitivity is informative of its $\tau$.

To see if sensitive issues display different rates of overall change, we fit a linear regression model where the outcome variable is overall change and the main independent variable is an issue's median sensitivity in each country. Given that the outcome variable is truncated at zero -- a variable cannot display less than no change -- we use the lognormal link.  As mentioned above, the model includes varying intercepts and slopes at the level of country. Panel A in figure X shows the posterior distribution of the population-level coefficient for sensitivity in this model, in the log scale^[Full model results as well as assessments of fit are included in the supplementary materials]. The distribution is centered around 0.15, and a considerable amount of the mass lies below 0. The uncertainty in this posterior distribution suggests that -- in our data -- there is not a strong relationship between an issue's perceived sensitivity and the amount of change it displays. 

```{r}
# Our child qualities question maps on to a few in the WVS
# Let's create those
d_sens_complete <- d_sens_complete %>% 
  mutate(
    child_hard_work = child_qualities_1, 
    child_feeling_responsibility = child_qualities_1, 
    child_imagination = child_qualities_1, 
    child_tolerance = child_qualities_1, 
    child_thrift = child_qualities_1, 
    child_determination = child_qualities_1, 
    child_religion = child_qualities_1, 
    child_unselfish = child_qualities_1, 
    child_obedience = child_qualities_1
  )

d_sens_long <- d_sens_complete %>% 
  select(country, imp_family_1:just_suicide_1, proud_1, child_hard_work:child_obedience) %>% 
  mutate(
    across( 
      .cols =2:62, 
      as.numeric
    )
  ) %>% 
  pivot_longer(
    2:62, 
    names_to = "question", 
    values_to = "difficulty"
  ) %>% 
  mutate(
    question = str_remove(question, "_1$"), 
    variable = case_when( 
      question == "imp_family" ~ "important_family", 
      question == "imp_friends" ~ "important_friends", 
      question == "imp_leisure" ~ "important_leisure", 
      question == "imp_pol" ~ "important_politics", 
      question == "imp_wrk" ~ "important_work", 
      question == "imp_rel" ~ "important_religion", 
      question == "child_qualities" ~ "child_independence", 
      question == "neigh_race" ~ "neigh_diff_race", 
      question == "neigh_drink" ~ "neigh_drink", 
      question == "neigh_imm" ~ "neigh_imm", 
      question == "neigh_aids" ~ "neigh_aids", 
      question == "neigh_drug" ~ "neigh_drugs", 
      question == "neigh_gay" ~ "neigh_gay", 
      question == "trust" ~ "trust_people", 
      question == "life_satisf" ~ "life_satisf", 
      question == "choice_control" ~ "choice_control", 
      question == "scarce_jobs_women" ~ "jobs_men_over_women", 
      question == "scarce_jobs_imm" ~ "jobs_national_over_foreign", 
      question == "housewife_fulfill" ~ "housewife_fulfilling", 
      question == "left_right" ~ "politics_scale", 
      question == "equal_incomes" ~  "income_eq", 
      question == "private_public_busi" ~ "pvt_state_owned", 
      question == "gvt_responsibility" ~ "gvt_responsibility", 
      question == "comp_good_bad" ~ "competition_good_evil", 
      question == "conf_church" ~ "confidence_churches", 
      question == "conf_armed" ~ "confidence_armed_forces", 
      question == "conf_press" ~ "confidence_press", 
      question == "conf_unions" ~ "confidence_unions", 
      question == "conf_police" ~ "confidence_police", 
      question == "conf_parliament" ~ "confidence_parliament", 
      question == "conf_civil" ~ "confidence_civil", 
      question == "conf_tv" ~ "confidence_tv", 
      question == "conf_gvt" ~ "confidence_government", 
      question == "conf_pol_prt" ~ "confidence_political_parties", 
      question == "conf_companies" ~ "confidence_major_companies", 
      question == "conf_courts" ~ "confidence_justice_courts", 
      question == "rel_services" ~ "attend_relig", 
      question == "religiosity" ~ "religious_person", 
      question == "belief_god" ~ "believe_god", 
      question == "belief_hell" ~ "believe_hell", 
      question == "imp_god" ~ "important_god", 
      question == "just_gvt_benefits" ~  "just_gvt_benefits", 
      question == "just_public_trans" ~ "just_fare_public_trans", 
      question == "just_cheat_tax" ~ "just_cheat_taxes", 
      question == "just_bribe" ~ "just_bribe", 
      question == "just_homo" ~ "just_homosexuality", 
      question == "just_prst" ~ "just_prostitution", 
      question == "just_abort" ~ "just_abortion", 
      question == "just_divorce" ~ "just_divorce", 
      question == "just_euth" ~ "just_euthanasia", 
      question == "just_suicide" ~ "just_suicide", 
      question == "proud" ~ "proud_nationality", 
      TRUE ~ question),
  )

# Center the results
d_sens_long <- d_sens_long %>% 
  group_by(
    country
  ) %>% 
  mutate(
    sens_ctd = scale(difficulty)[,1]
  ) %>%  
  ungroup()

# Now calculate the median and mean
summary_sens <- d_sens_long %>% 
  group_by(country, 
           variable) %>% 
  summarise(
    avg = mean(sens_ctd, na.rm = T), 
    med = median(sens_ctd, na.rm = T)
  ) %>% 
  ungroup()

results_sens <- results %>% 
  left_join(., summary_sens, by = c("country", "variable"))

saveRDS(results_sens, 
        "data_for_analysis.rds")

set.seed(43543)

output1 <- capture.output(mod1 <- brms::brm(abschange ~ (med | country) + med, 
                  data = results_sens, 
                  family = "lognormal"))

saveRDS(mod1, "model_abschange.rds")

mix <- mixture(Beta, Beta)

output2 <- capture.output(mod2 <- brm(bf(pbetween ~ 1, 
               theta2 ~ (med | country) + med), 
            results_sens, 
            family = mix,
            chains = 4, 
            iter = 2000))

saveRDS(mod2, "model_beta.rds")

output3 <- capture.output(mod3 <- brm(pbetween ~ (med | country) + med, 
            results_sens, 
            chains = 4, 
            iter = 2000))

saveRDS(mod3, "model_linear.rds")

prds_mod1 <- prepare_predictions(mod1)["dpars"]

panel_a <- data.frame(prds_mod1$dpars$mu$fe$b) %>% 
  ggplot(
    aes(x = b_med)
  ) +
  geom_density(fill = "gray80", 
               alpha = 0.8) +
  geom_vline(aes(xintercept = 0), 
             linetype = "dashed", 
             col="darkred") +
  labs(y = "", 
       x = "Estimate of Sensitivity (log scale)", 
       title = "Panel A") +
  theme(text = element_text(size = 8)) 

panel_b <- results_sens %>% 
  ggplot(aes(x = pbetween)) +
  geom_density(fill = "gray80", 
               alpha = 0.8) +
  labs(y = "", 
       x = "Distribution of Tau", 
       title = "Panel B") +
  theme(text = element_text(size = 8)) 

prds_mod2 <- prepare_predictions(mod2)["dpars"]

panel_c <- data.frame(prds_mod2$dpars$theta2$fe$b) %>% 
  ggplot(
    aes(x = b_theta2_med)
  ) +
  geom_density(fill = "gray80", 
               alpha = 0.8) +
  geom_vline(aes(xintercept = 0), 
             linetype = "dashed", 
             col="darkred") +
  labs(y = "", 
       x = "Estimate of Sensitivity (log-odds scale)", 
       title = "Panel C") +
  theme(text = element_text(size = 8)) 

panel_a + panel_b + panel_c
```
In models with varying slopes and intercepts, it is difficult to interpret population-level effects, so it is more intuitive to plot the predictions that the model implies. Figure X shows these predictions with the x-axis representing centered sensitivity scales ranging from -1.5 standard deviations below the mean and 1.5 standard deviations above. The lines represent the mean prediction for each value of sensitivity. We notice that, while the relationship appears to be positive in countries like Argentina and South Africa, it is  flat in the rest of our sample. Thus, we find no compelling evidence to suggest a specific relationship between an issue's sensitivity and the amount of change it exhibits and, therefore, it is not possible to draw  any strong conclusions.

```{r}
newdata <- expand_grid(
  country = unique(results_sens$country), 
  med = seq(from =-1.5, to= 1.5, by=0.05)
)

newdata$preds <- predict(mod1, newdata)

newdata %>% 
  ggplot(aes(x = med, 
             y = preds[,1]), 
         fill = country)  +
  geom_line(linetype = "dashed")  +
  geom_point(
    data = results_sens, 
    aes(x = med, 
        y = abschange), 
    pch = 23
  ) +
  facet_wrap(~country) + 
  labs(
    title = "Absolute Change and Sensitivity", 
    y = "Abs. Change", 
    x = "Centered Sensitivity"
  )
```

To model $\tau$ as a function of the sensitivity of the issues, it is necessary to make a few additional considerations. First, given that $\tau$ is a proportion, it is bounded between 0 and 1. Second, in our data, $\tau$ exhibits a clear bi-modality. We address this by fitting a finite-mixture model where we consider $\tau$ as produced by two different beta distributions, themselves bounded between 0 and 1. Panel B of Figure X shows the bi-modal distribution of $\tau$, which we are going to model as consisting of two beta distributions. 

Within the model, we also regress the probability of an issue belonging to the distribution with higher $\tau$ on that issue's perceived sensitivity. In other words, we ask the question: does the perceived sensitivity of an issue tell us whether it is more likely to have emerged from the beta distribution with higher average $\tau$? If this is the case, then a higher sensitivity should be associated with a larger proportion of change explained by *between-cohort differences*. As above, in the regression component, we include varying intercepts and slopes at the level of country.^[We also perform this analysis using gaussian linear regression and the results are substantially the same. We include those analyses in the supplementary materials]

Panel C in Figure X  displays the posterior distribution for the population-level coefficient for the effect of sensitivity on the probability of belonging to the distribution with higher $\tau$^[Full model results as well as assessments of fit are included in the supplementary materials]. The coefficient is in the log-odds scale, but it is readily apparent that the majority of its mass lies above 0. The model suggests then that as an issue's sensitivity increases, so does the probability that it belongs to the distribution with higher average $\tau$. 

However, results in the log-odds scales are notoriously difficult to interpret and given the overall complexity of the model, it is better to examine its predictions to understand the implications of the results. Figure X displays the models predicted outcomes, where the lines represent the average prediction at each value of sensitivity . We notice that the slopes consistently exhibit a slight, positive relationship. Again, we have variation between the countries: while a one standard deviation increase in sensitivity in Mexico leads to a predicted increase in $tau$ of around 0.158, in Sweden the increase is of around 0.147. Despite this variation, the evidence for a positive relationship is broadly consistent across countries. Thus, although the effect of sensitivity is not large, and there are differences across countries, our model suggests that -- on average -- we should expect more sensitive issues to change more through *between-cohort differences* as opposed to *within-cohort changes*.  

```{r}
newdata <- expand_grid(
  country = unique(results_sens$country), 
  med = seq(from =-1.5, to= 1.5, by=0.05)
)

newdata$preds <- predict(mod2, newdata)

newdata %>% 
  ggplot(aes(x = med, 
             y = preds[,1]), 
         fill = country) +
  # geom_ribbon(aes(
  #   ymin = preds[,3], 
  #   ymax = preds[,4]
  # ), 
  # col = "gray80", 
  # alpha = 0.1) +
  geom_line(linetype = "dashed") +
  geom_point(
    data = results_sens, 
    aes(x = med, 
        y = pbetween), 
    pch = 23
  ) +
  facet_wrap(~country) + 
  labs(
    title = "Tau and Sensitivity", 
    y = "Pbetween", 
    x = "Centered Sensitivity"
  )


```

## Discussion

In this paper, we expand on previous work on large-scale cultural change by examining multiple cultural contexts, and by systematically examining *what* variables have changed in each setting and *how*. The last decade has seen renewed interest in one of the central theoretical questions in the social sciences: how societies change. There have been important steps in understanding this puzzle. At the aggregate level, there is evidence that most average intra-cohort opinions remain relatively stable, which implies that large-scale change most likely occurs through cohort replacement. At the individual-level, previous work shows that although adults do sometimes change their beliefs, they mostly remain stable in their attitudes throughout their lives. This -- again -- reinforces the idea that the main mechanism of aggregate-level change is most likely cohort succession. What previous research has not done is move beyond a single cultural context to make systematic comparisons between different settings, and try to adjudicate how different types of variable change across cultures and groups. We follow the methods that analyze aggregate-level change to try to address these gaps.

Building on previous research on cultural change, we seek to adjudicate the mechanisms that drive processes of large-scale cultural change. We note that there are two broad ways in which we can understand change at the individual level across the life-course: the "active-updating" model and the "settled-dispositions" model. The former portrays individuals as continuously open to new information, and the latter notes that, after the critical period of socialization, individuals are mostly stable in their attitudes. At the aggregate level, both models predict widely different patterns of social change. If individuals are active updaters, we would expect change to happen swiftly, as whole sections of the population react to new information and/or circumstances. That is, we should expect change to b driven largely by *within-cohort changes*. In turn, if individuals are mostly stable in their attitudes after adolescence, then change should be gradual, as younger cohorts replace their predecessors. Here, change would mostly be explained by *between-cohort differences*. We contend that we can begin to adjudicate which one of these two models is most likely to be driving change in a given variable by examining how much variance is explained by two different regressions. In an idealized world, where all change is driven by cohort replacement, knowing the current year does not give us any information about attitudes at the aggregate level. On the other hand, if active-updating is driving change, then knowing the current year -- and therefore the current circumstances -- is crucial for understanding current beliefs at the level of the population. Our measure $\tau$ captures this relationship: it represents the proportion of variance explained that is preserved when the effect of time is taken out of the model. In other words, a higher $\tau$ would suggest that most of the variance in certain issue is explained by *between-cohort differences* rather than *within-cohort changes*.

We not only want to examine the explanatory power of each mechanism, but we want to understand if different types of variables are changing due to different mechanisms. We contend that the idea of sensitivity allows us to get an initial handle on this question. Sensitive issues like gay civil rights and abortion have displayed change across many contexts in the last several decades. These changes have been contentious and have resulted in acrimonious divisions and political backlash. Sensitive issues are paradoxical in a way: at the aggregate level they have displayed considerable change, we have evidence of individual attitudinal updating around them, and yet they seem like the beliefs where agents are most firmly entrenched. Previous work, then, suggests that these issues might change differently than other topics that are considered perhaps more mundane.

Operationalizing the concept of sensitivity, however, is difficult. In this paper, we tackled this challenge by collecting data from eight countries about how difficult it may be for people in those settings to discuss certain issues. Using the online marketplaces Prolific and CloudResearch, we collected data from 802 participants across the countries of interest. Each participant was presented with the survey questions that accompanied the relevant variables in the WVS -- translated to the appropriate language. Their task was to rate how difficult it would be for the *majority of people from their countries* to discuss the question at hand. While our data is not representative, we believe that the strategy of prompting participants to think about second-order beliefs yielded adequate approximations of perceptions of sensitivity in each setting. This is illustrated by the fact that our data captures general patterns of sensitivity -- e.g. around issues like abortion -- as well as country-specific trends. 

Our results capture context-specific dynamics across different countries, while also revealing important commonalities in processes of cultural change. For instance, we show that stability is quite common across most of the issues we examine. Across most countries, attitudes towards homosexuality exhibit considerable change, and this change can be mostly attributed to *between-cohort differences*. In some countries, we see a similar pattern for other sensitive issues like attitudes around divorce and euthanasia. The variables that display more variation through *within-cohort change* tend to be related to confidence in institutions and the characteristics that are desirable in children. These issues also seem to -- at times -- capture important historical processes, like the restructuring of the armed forces in Japan.

It is worth briefly discussing why issues like confidence in institutions would display more *within-cohort change*. These institutions change in meaning as their occupants come and go. Thus, what it means to support -- or have confidence in -- an institution might change depending on who currently occupies it. For a Democrat in the U.S., for example, the election of Donald Trump might have considerably changed their confidence in the presidency, even though none of their attitudes really varied. Similarly, a conservative Catholic in that country might currently look favorably at a Supreme Court that they might have distrusted a few decades ago. We would not say these individuals have changed their minds -- at least not in the ways that are most interesting to social scientists. What has changed is the meaning associated with those institutions.

We explore differences between more or less sensitive issues more rigorously by examining whether our data about sensitivity is predictive of the extent to which variables have changed and how they have changed. We find mixed evidence about whether the sensitivity of issues is informative of how much they change. While we see a positive relationship in Argentina and South Africa, the relationship is non-existent in the other settings. This uncertainty prevents us from drawing strong conclusions about whether sensitive issues change more or less than their counterparts. Nonetheless, our analysis suggests that sensitive issues tend to change more through *cohort replacement* than through *within-cohort change*. Our model identifies a relationship between an issue's sensitivity and the proportion of its variation that is explained by *between-cohort differences*. The relationship is slight, but it is consistent across the settings in our sample. We find consistent evidence, then, that issues that are difficult to talk about change more privately.

Now, this seems like an intuitive conclusion but it has important implications. First, it provides initial evidence that different types of variables change -- at the aggregate level -- through different mechanisms. Second, it sheds light on current heated discussions about the sensitive issues that are the center of public debate. In his work about transnational LGBT+ rights, Velasco [-@velascoTransnationalBacklashDeinstitutionalization2023] shows that there has been considerable backlash in reaction to some of the political victories of this community. Through the lens of attitudinal change, this presents a puzzle: how do we reconcile the variation in attitudes towards homosexuality with this forceful backlash? Well, if most of that change has occurred through cohort replacement, it means that older individuals, many who occupy positions of power, have not been swayed in their opposition to the expansion of civil rights for the LGBT+ community. As younger cohorts move further away from their predecessors, we should still expect the fight to be acrimonious while the latter are still a considerable -- but perhaps dwindling -- portion of the civil sphere. Moreover, we should expect these divisions to be particularly salient in contexts where older cohorts hold more anti-LGBTQ+ views, and therefore where inter-cohort average opinions might be further away. This is reflected in Velasco's cross-cultural analyses. The fact that some issues are difficult to talk about, then, might mean that when these topics change, they might display both considerable variation at the aggregate level and entrenchment at the individual level.

Our study, then, helps widen ongoing conversations about the mechanisms of cultural change by contrasting different variables and national contexts. We contend that, at the heart of current debates about cultural change, lies the question of the relative explanatory power of *within-cohort changes* and *between-cohort differences*. We outline a straightforward method to adjudicate which one of these sources of variance explains most change at the aggregate level for a variable. Then, we carry out this test for 56 different variables across 8 different countries. We find that common patterns -- like changes in attitudes towards homosexuality -- as well as processes particular to each context, like changes in public trust towards the armed forces in Japan. We also find variation in what kind of processes underpin change in different variables. For attitudes towards homosexuality, for example, *between-cohort changes* appears to be an important mechanism of change. For issues that relate to confidence in institutions, we find that *within-cohort changes* explain a lot of the variation evidenced at the group level. In general, we find evidence that suggests that more sensitive issues tend to change more through cohort replacement. In other words, themes that are more difficult to talk about change more privately. This represents a theoretically-informed finding that suggests that different issues might change through distinct mechanisms. This is an important finding for scholars of cultural change, because it suggests that different aspects of culture can change through varying mechanisms and at varying paces. It is key then to begin to outline clear and testable theoretical statements that link these different aspects of culture with the mechanisms of change that might undergird them. Here, we suggest that one such mechanism is how difficult it may be to talk about certain subjects. These issues should change more through cohort replacements, as people are less likely to change their already established positions.

## References
