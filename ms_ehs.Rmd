---
title: "Opinions on Hard-to-Discuss Topics Change More via Cohort Replacement"
author: 
  - Nicolas Restrepo Ochoa[^NRO]
  - Stephen Vaisey[^SV]
output:
  pdf_document:
  html_document:
    toc: yes
    theme: united
header-includes:
- \usepackage{setspace}
- \doublespacing
- \usepackage{lineno}
- \linenumbers
- \def\linenumberfont{\normalfont\tiny\sffamily}
bibliography: sensitive_issues.bib
abstract: "Cohort replacement---the replacement in a population of older cohorts by their successors who developed under different conditions---is an important process behind cultural change. Research on public opinion indicates that a large proportion of aggregate change is the result of cohort replacement rather than of individuals changing their minds. However, some publicly salient issues, like gay rights, appear to be exceptions. Why different issues show different patterns of change is not well understood. In this paper, we investigate whether opinions on sensitive---that is, hard to discuss---issues might change differently than opinions on less sensitive issues. We use data from the 1981-2020 World Values Surveys and newly collected data on the sensitivity of survey items to compare aggregate changes in public opinion on 56 survey items in 8 countries. Our key finding is that survey items on more sensitive issues seem to change more through cohort replacement."
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, message = F, warning = F)

library(tidyverse)
library(countrycode)
library(janitor)
library(splines)
library(performance)
library(ggrepel)
library(hrbrthemes)
library(patchwork)
library(broom)
library(broom.mixed)
library(gtsummary)
library(labelled)
library(brms)
library(kableExtra)
theme_set(theme_bw())
```

[^NRO]: Anthropology Department, University of California at Davis.  
[^SV]: Sociology Department, Duke University.

Words: `r wordcountaddin::word_count("manuscript.Rmd")`

\newpage

## Introduction

Much of the scholarly interest in culture lies in trying to understand how it changes. Cultural trends like secularization [@bergerSecularizationDesecularization2002; @chavesSecularizationDecliningReligious1994; @tormosRhythmModernizationHow2021] or the rise of (and backlash against) gender equality [@velascoTransnationalBacklashDeinstitutionalization2023] continue to receive sustained academic attention. Recent work on opinion change in the U.S. suggests that most changes happen primarily---although not exclusively---through cohort replacement [@vaiseyCulturalFragmentationAcquired2016a; @kileyMeasuringStabilityChange2020]. That is, culture usually changes as young people, who grew up in different social conditions, replace those that came before them. Nonetheless, beliefs about some salient issues, like gay rights, seem to be exceptions, where individuals appear to be changing their minds well into adulthood [@kileyMeasuringStabilityChange2020; @tormosRhythmModernizationHow2021]. This suggests that different beliefs might change via different underlying mechanisms. This is what we investigate in this paper.

We have two main goals. First, we examine whether there are systematic differences in patterns of change across variables, especially in relation to how sensitive---that is, how difficult to discuss---they are. Individuals do update their beliefs about particularly salient issues; however, these issues tend to be difficult to talk about because interlocutors are often firmly entrenched in their beliefs. The sensitivity of a topic might be a useful gateway to start asking how different beliefs change via different mechanisms. Second, following the work of Törmos and colleagues [-@tormosRhythmModernizationHow2021; -@tormosPatternsChangeJustifiability2023], we want to move beyond U.S. data and examine cross-cultural variation in how different issues change. We approach this comparative work slightly differently, using methods that have – up to date – mainly been implemented to explore change in the U.S. context. In short, we want to dig deeper into the mechanisms that underpin change for different cultural issues, and we want to examine whether these mechanisms of change are consistent across contexts.

We investigate these questions using data from the World Value Survey (WVS) [@inglehartWorldValuesSurveys2000]. Though the data are cross-sectional and therefore cannot directly answer questions about individual-level change, we can use them to adjudicate between different mechanisms that might account for population-level cultural change. To do this, we propose a straightforward method. We begin by positing an idealized model, where, after the critical period of youth, individuals do not change their beliefs. Under these assumptions, all cultural change can be explained by *between-cohort differences* and all we need to know to estimate a person's opinion on a given issue is to know their year of birth. Then, we consider another model, where the average opinion of each cohort is allowed to change linearly over time. We contend that, when fitted to the WVS data, the relative explanatory power of these two models provides an indication of the mechanism that might be responsible for change for a given issue. If the proportion of the variance explained is relatively unchanged when we move from the first model to the second one, then this suggests a given issue is changing primarily through cohort replacement. If the second model improves greatly on the first, then this would point towards the importance of *within-cohort change* in accounting for the trends.

To examine patterns of change across different types of cultural issues, we fit these models to 56 different variables, measured between 1981-2020, across 8 countries. We select the countries -- Argentina, Australia, Canada, Japan, Mexico, South Africa, Sweden, and the USA -- based on completeness, seeking to cover the longest time-spans possible with this survey. We also choose to focus on variables that have been asked in all the waves of the WVS, and that cover a wide-range of topics and different levels of sensitivity, from the justifiability of euthanasia to whether imagination is a desirable attribute in children.

Our results lead to several key insights. We show that, consistent with previous work on opinion change over the past five decades, large differences are uncommon. Nonetheless, echoing work on attitudinal change in the U.S. [@kileyMeasuringStabilityChange2020] and previous work using the WVS [@tormosRhythmModernizationHow2021], we find that the variable that has changed most consistently across the countries is related to attitudes about homosexuality. Furthermore, we show that cohort replacement explains a considerable portion of the variation in some of the variables that display the most linear change. Perhaps most relevant to our questions about the mechanisms of change, we see a pattern across the sensitivity of different cultural issues. We find that change in more sensitive topics can be explained mostly by *between-cohort differences*, and variation in less sensitive issues can be attributed more to *within-cohort change*. This provides some evidence for the claim that issues change through different mechanisms and provides a starting point for identifying which issues are more likely to change in different ways. All necessary code and data to reproduce this paper is available on: <https://github.com/NicolasRestrep/sensitive_change> .


## Cultural change and its elements

Cultural change has been a central preoccupation of social scientists. Recently, as new data sources with longer time series have become available, there has been a renewed interest in trying to understand cultural change quantitatively. This newer work has focused on linking individual mechanisms of belief updating to large-scale processes of change [@kileyMeasuringStabilityChange2020; @keskinturkReligiousBeliefAlignment2021; @bartelsGenerationalModelPolitical2014; @tormosRhythmModernizationHow2021]. 

When it comes to matters of beliefs and attitudes, *societies* do not change; *individuals* change, as does the composition of individuals in a population. The aggregation of those individual beliefs is what can we measure as shifts at the population level. Therefore theories of large-scale cultural change are -- at their core -- accounts of how individuals update their beliefs and habits.

Large-scale change can occur through several individual-level mechanisms. These mechanisms are generally classified into *age effects*, *period effects*, and *cohort effects* [@fosseAnalyzingAgeperiodcohortData2019].

Age effects are are reactions to one's personal "biography." For instance, a citizen might veer away from direct action and radical politics as they accrue wealth and have more to lose in the case of a structural societal change [@mcadamBiographicalConsequencesActivism1989].

Period effects are the result of new information or events that affect an entire population at the same time. During a time for war, for example, we might expect individuals -- across all age-groups -- to change how they view the armed forces or their country in general. 

Cohort effects are the enduring effects of certain historical moments---such at the Great Depression---that leave a mark on individuals who grow up under those conditions [@elderChildrenGreatDepression2018a]. These individuals, then, would have distinctive beliefs and attitudes that they would carry throughout their lives [@ryderCohortConceptStudy1965; @elderChildrenGreatDepression2018a; @fosseAnatomyCohortAnalysis2023].

Disentangling these different sources of cultural change is a well-known challenge [@bellImpossibilitySeparatingAge2013]. In the framework of a standard regression with cross-sectional data, it is---strictly speaking---impossible. These three sources of variation are perfectly collinear because, if we know an individual's age and the current year, we also know precisely when they were born [@fosseAnalyzingAgeperiodcohortData2019]. Though this is not the place to provide a full review of the work on age-period-cohort effects (cf. [@fosseAnalyzingAgeperiodcohortData2019; @tormosRhythmModernizationHow2021]), it suffices to mention that researchers have devised several strategies to disaggregate these three sources of change. Nonetheless, all strategies involve a kind of arbitrary compromise, like assuming quadratic age effects or binning cohorts into differently sized groupings [@vaiseyCulturalFragmentationAcquired2016a]. 

Fortunately, given our questions, we do not need to disentangle all three types of effects. As we explain in the next section, the most important theoretical distinction is between a model that contains only between-cohort differences (i.e., cohort effects), and a model that includes within-cohort change (resulting from either age or period effects).

## Two models of individual-level change

There are two broad theoretical models of individual change. The first is the "settled dispositions" model [@kileyMeasuringStabilityChange2020; @underwoodCohortSuccessionExplains2022]. This model posits that an individual's beliefs develop during a critical period of socialization. After one's formative years, therefore, beliefs generally remain stable. This has the further implication that individuals raised in similar socio-historical contexts will share certain beliefs and attitudes that they carry throughout their lives [@elderChildrenGreatDepression2018a; @gerberRationalLearningPartisan1998; @ryderCohortConceptStudy1965]. 

The second model is the "active updating" model. It assumes that individuals remain open to revising their beliefs across the life course. This model is related to social theories that portray the self as continuously under construction [@grossPragmatistTheorySocial2009]. On this view, people are open to novel information -- including biographical information gained through aging -- and therefore sensitive to changes in their cultural and social contexts. This implies that cultural moments would play a much bigger role in shaping individuals' attitudes [@tormosRhythmModernizationHow2021]. In other words, individuals will reconsider their attitudes in light of the cultural trends and/or political movements happening at a particular historical moment. Visions of historical change as changes in the *zeitgeist* rely implicitly on the idea that individuals are attuned to the "spirit of the age", ready to change their beliefs with the times.

Although no scholars believe that either model provides a complete account of change, attempts to compare the explanatory power of the two models has been at the center of research about large-scale change for the past two decades [@vaiseyCulturalFragmentationAcquired2016a; @tormosRhythmModernizationHow2021]. In practical terms, Lizardo and Vaisey [-@vaiseyCulturalFragmentationAcquired2016a] argue that the differences between these models can be boiled down to a rather simple question: *to predict a person's attitudes are we better off knowing the current year or their date of birth?*

Although, in practice, the question is not quite that simple, the settled dispositions and active updating models have different implications for the patterns we should expect to see at the population level over time. If the active updating model were the dominant process, we would expect cultural change to happen swiftly, following particular events or shocks [@tormosRhythmModernizationHow2021]. For example, an unexpected economic downturn might lead the members of a group -- regardless of age -- to be more conservative in their financial choices. A series of catastrophic climate disasters might lead them to update their beliefs on human-induced climate change. In other words, exogenous changes will be reflected directly in aggregate cultural attitudes, as the population updates their attitudes in light of new information or new circumstances. 

The settled dispositions model paints a rather different picture. It assumes that individuals beyond their formative years will be less swayed by exogenous changes. Thus, cohorts raised under unfavorable economic circumstances or during a climate crisis will develop attitudes based on these formative experiences even if the external environment later changes. Thus aggregate beliefs will only change as earlier cohorts, raised under different circumstances, die and are replaced [@ryderCohortConceptStudy1965]. Aggregate social change in this scenario will tend to be more gradual.

Recent work on belief change suggests that the settled dispositions model---although incomplete---is a better default model for explaining aggregate social change [@vaiseyCulturalFragmentationAcquired2016a; @kileyMeasuringStabilityChange2020; @underwoodCohortSuccessionExplains2022]. Lizardo and Vaisey [-@vaiseyCulturalFragmentationAcquired2016a], for instance, compare the explanatory power of both models in cross-sectional time-series data from the U.S. They find that most beliefs remain relatively stable within cohorts, supporting the idea that aggregate change is best modeled as cohort succession. Analyses of panel data provides additional support that adults generally do not change their minds on issues over time [@kileyMeasuringStabilityChange2020; @bartelsGenerationalModelPolitical2014]. As the settled dispositions model predicts, cohorts (and individuals) seem to remain generally stable on most issues over time.

## Variation in mechanisms of change across issues

The claim that cultural change occurs *primarily* through cohort replacement does not mean, of course, that this is the *only* mechanism of change. Recent work by Lersch [-@lerschChangePersonalCulture2023] shows that individuals do change in adulthood, even if observed changes are small in magnitude relative to persistent between-person differences. Kiley and Vaisey [-@kileyMeasuringStabilityChange2020] also show that there are certain issues where we observe durable change among adults. In the U.S., for instance, there is evidence of intraindividual updating on beliefs about homosexuality, a particularly salient issue for the past few decades in the United States. Törmos [-@tormosRhythmModernizationHow2021] also finds that, across the OECD countries, cohorts also exhibit considerable change in their opinions towards homosexuality.

To this point, researchers have focused on general patterns, often counting the number of survey items for which different models provide better statistical fits to data [@vaiseyCulturalFragmentationAcquired2016a; @kileyMeasuringStabilityChange2020; @lerschChangePersonalCulture2023]. They also note exceptions (such as gay rights). But to advance the science of cultural change, we have to investigate systematically *why* beliefs about different issues appear to change via different processes rather than telling just-so stories. What is it about some issues that makes beliefs about them more likely to change even in adulthood? And are there cross-cultural differences in what these issues are?

The number of attributes that might vary across topics is essentially infinite. However, we believe that the concept of *sensitivity* might allow us to get an initial handle on this problem. Campbell and Mace (this issue) define sensitive issues as issues that are difficult to talk about. In most surveys, questions vary a great deal in how sensitive they are, from the importance of friends in your life (perhaps not very sensitive) to the justifiability of suicide (perhaps quite sensitive). In the rest of the paper, we investigate whether answers to more sensitive questions show evidence for different change mechanisms than answers to less sensitive questions.

We believe that answers to questions about more sensitive issues will change more slowly (i.e., more by cohort replacement) than answers to questions about less sensitive issues. We believe this for two reasons. The first reason is psychological. The very sensitivity of the issues might mean that they constitute key elements in individuals' worldviews. Beliefs on these issues might not open for discussion or revision. 

The second reason is interactional. Sensitive issues are difficult to talk about and thus we talk about them less often or only with a few others. We gain less information about what other individuals believe, and thus external cues that might prompt reexamination are hard to come by. This would result in a scenario akin to pluralistic ignorance [@halbeslebenPluralisticIgnoranceHistorical2004; @j.ogormanDiscoveryPluralisticIgnorance1986], where individuals’ reticence to discuss certain topics precludes active conversations that might lead to attitudinal updating. 

Both mechanisms lead to a similar conclusion: we should expect beliefs about more sensitive issues to change more slowly. This would mean that the changes we observe in beliefs on these topics at the aggregate level should be mostly attributed to cohort replacement. 

## Methods
### Disentangling within-cohort and between-cohort differences

Our discussion above suggests that the goal is not disentangling the full range of age, period, and cohort effects, but rather adjudicating the relative explanatory power of the two broad models of individual-level updating. This objective is simpler and more attainable. If the settled dispositions model is dominant, then we should expect most cultural change to be driven by differences between cohorts. In turn, if the active updating model is more explanatory, then we should see evidence of considerable changes within cohorts, as they age and as they experience new information and events. The central distinction, then, is between the relative importance of *between-cohort differences* and *within-cohort change*, with temporary *period effects* and biographical *age effects* subsumed in the latter.

To clarify the distinction between patterns of large-scale change mainly driven by *between-cohort differences* or *within-cohort change*, it is useful to envision two idealized models of aggregate change. First, imagine a scenario where, after the critical period of socialization, cohorts have formed beliefs from which they do not deviate. If we were able to track the data by cohort it would look like overlapping horizontal lines, with different intercepts on the y-axis. Change, at the aggregate level, would look like a gradual shift towards the averages of the younger cohorts. Figure 1 illustrates both dynamics. In this case, knowing a person's year of birth would give us a good estimate of their opinion, in whatever year and at whatever age it what measured. Cohort differences would also explain all the variation in aggregate change, given that -- in this idealized scenario -- all change occurs through cohort replacement.


```{r, fig.cap="Cohort trends (a) & aggregate change (b) for an idealized model with no within-cohort changes"}
set.seed(2607)
cohort_1920 <- tibble(
  year=(1920+18):2000,
  opinion=rnorm(length(year), 0.1, sd = 0.1)
)
cohort_1940 <- tibble(
  year=(1940+18):2020,
  opinion=rnorm(length(year), 0.3, sd = 0.1)
)
cohort_1960 <- tibble(
  year=(1960+18):2020,
  opinion=rnorm(length(year), 0.5, sd = 0.1)
)

all_cohorts <- rbind(cohort_1920, cohort_1940, cohort_1960)

cohort_plot <- all_cohorts %>%
  mutate(cohort = rep(c("1920",
                    	"1940",
                    	"1960"),
                  	times = c(63, 63, 43))) %>%
  ggplot(aes(x = year,
         	y = opinion,
         	color = cohort,
         	group = cohort)) +
  geom_point(pch = 23) +
  geom_segment(aes(xend = 2020,
               	x = 1978,
               	y = 0.5,
               	yend = 0.5),
           	col = "blue") +
  geom_segment(aes(xend = 2020,
               	x = 1958,
               	y = 0.3,
               	yend = 0.3),
           	col = "forestgreen") +
  geom_segment(aes(xend = 2000,
               	x = 1938,
               	y = 0.1,
               	yend = 0.1),
           	col = "red") +
  labs(y = "Opinion",
   	x = "Year",
   	color = "Cohort",
   	title = "(A)")


aggregate_change <- all_cohorts %>%
  mutate(cohort = rep(c("1920",
                    	"1940",
                    	"1960"),
                  	times = c(63, 63, 43))) %>%
  group_by(year) %>%
  summarise(avg = mean(opinion)) %>%
  ggplot(
	aes(x = year,
    	y = avg)
  ) +
  geom_point(pch = 23) +
  geom_line(color = "firebrick") +
  labs(
	y = "Avg. Opinion",
	x = "Year",
	title = "(B)"
  )

p1 <- cohort_plot + aggregate_change
p1 

ggsave("~/Documents/sensitive_change/aggregate_no_within.png", p1, dpi = 500)
```


Now, imagine another scenario where adults do update their beliefs, either because as individuals get older they tend to change their beliefs or because an issue has been particularly salient in public discussions. In other words, we would assume that there are, in addition to initial *between-cohort differences*, *within-cohort changes*, which can be either *period* or *age* effects (for our purposes, this distinction is unimportant). In this stylized example, we can imagine an issue -- like attitudes towards homosexuality -- that has become increasingly important in the public sphere since the middle of the 20th century and where individuals seem to have updated their beliefs. Figure 2 shows this second example. Here we see *within cohort changes*, due to common trends experienced by all members of the group. This, in turn, translates into much steeper cultural change at the aggregate level. Cultural change here is not only due to the overall differences between cohorts - and their replacement - but also due to changes in the same direction within cohorts.


```{r, fig.cap="Cohort trends (a) & aggregate change (b) for an idealized model with both within-cohort changes & between-cohort differences"}

coh_plot_period <- all_cohorts %>%
  mutate(cohort = rep(c("1920",
                    	"1940",
                    	"1960"),
                  	times = c(63, 63, 43)),
     	opinion = if_else(
       	year <1960,
       	opinion,
       	opinion + (year-1938)*0.01
     	)) %>%
  ggplot(aes(x = year,
         	y = opinion,
         	color = cohort,
         	group = cohort)) +
  geom_point(pch = 23) +
  geom_smooth(method = "lm",
          	se=F) +
  labs(y = "Opinion",
   	x = "Year",
   	color = "Cohort",
   	title = "(A)")
 

aggregate_change_period <- all_cohorts %>%
  mutate(cohort = rep(c("1920",
                    	"1940",
                    	"1960"),
                  	times = c(63, 63, 43)),
     	opinion = if_else(
       	year <1960,
       	opinion,
       	opinion + (year-1938)*0.01
     	)) %>%
group_by(year) %>%
  summarise(avg = mean(opinion)) %>%
  ggplot(
	aes(x = year,
    	y = avg)
  ) +
  geom_point(pch = 23) +
  geom_line(color = "firebrick") +
  labs(
	y = "Avg. Opinion",
	x = "Year",
	title = "(B)"
  )

p2 <- coh_plot_period + aggregate_change_period
p2 
ggsave("~/Documents/sensitive_change/aggregate_within_between.png", p2, dpi = 500)
```


A more extreme variant of this example would be one where all cohorts start from the same average opinion -- regardless of the current year -- and experience the same *within-cohort changes*. In other words, we can imagine a scenario where there are no initial *between-cohort differences* and all age-groups follow the same trends in opinion change. Figure 3 illustrates such a case:


```{r, fig.cap="Cohort trends (a) & aggregate change (b) for an idealized model with no between-cohort differences"}


set.seed(2607)
cohort_1920 <- tibble(
  year=(1920+18):2000,
  opinion=rnorm(length(year), 0.1, sd = 0.05)
)
cohort_1940 <- tibble(
  year=(1940+18):2020,
  opinion=rnorm(length(year), 0.1, sd = 0.05)
)
cohort_1960 <- tibble(
  year=(1960+18):2020,
  opinion=rnorm(length(year), 0.1, sd = 0.05)
)

all_cohorts_same <- rbind(cohort_1920, cohort_1940, cohort_1960)

coh_plot_period <- all_cohorts_same %>%
  mutate(cohort = rep(c("1920",
                    	"1940",
                    	"1960"),
                  	times = c(63, 63, 43))) %>%
  group_by(cohort) %>%
  mutate(year0 = year-min(year)+1,
     	opinion = opinion + year0 * 0.01) %>%
  ungroup()  %>%
  ggplot(aes(x = year,
        	y = opinion,
        	color = cohort,
        	group = cohort)) +
  geom_point(pch = 23) +
  geom_smooth(method = "lm",
          	se=F) +
  labs(y = "Opinion",
   	x = "Year",
   	color = "Cohort",
   	title = "(A)")

aggregate_change_period <- all_cohorts_same %>%
  mutate(cohort = rep(c("1920",
                    	"1940",
                    	"1960"),
                  	times = c(63, 63, 43))) %>%
  group_by(cohort) %>%
  mutate(year0 = year-min(year)+1,
     	opinion = opinion + year0 * 0.01) %>%
  ungroup() %>%
  group_by(year) %>%
  summarise(avg = mean(opinion)) %>%
  ggplot(
	aes(x = year,
    	y = avg)
  ) +
  geom_point(pch = 23) +
  geom_line(color = "firebrick") +
  labs(
	y = "Avg. Opinion",
	x = "Year",
	title = "(B)"
  )

p3 <- coh_plot_period + aggregate_change_period
p3
ggsave("~/Documents/sensitive_change/aggregate_no_between.png", p3, dpi = 500)
```


Based on these idealized models, we propose a simple comparison that can help quantify the relative contribution of *within-cohort change* and *between-cohort differences*. We fit two models to the same data. 

The first model is a regression where the outcome variable is regressed only on the cohort of each respondent:

$$ y_i \sim N(\mu, \sigma^2) $$
$$ \mu = \alpha + \beta_{\text{cohort}[i]} $$

The second model adds a linear term for year (to allow for secular within-cohort change) and an interaction between cohorts and year, which allows every cohort to change in a different way:


$$ y_i \sim N(\mu, \sigma^2) $$
$$ \mu = \alpha + \beta_{\text{cohort}[i]} + \phi \text{ year}_i + \zeta_{\text{cohort[i]}} \text{ year}_i $$

The use of linear within-cohort time trends here requires some justification. This assumption means that we are unable to capture within-cohort fluctuations that might be caused by temporary shocks -- e.g., increased national pride during a national holiday or the Olympics -- that leave no lasting effect on aggregate opinion. While we acknowledge that these fluctuations are a part of within-cohort *variation*, they cannot account for monotonic aggregate changes over time. When social scientists discuss cultural change, they typically mean *directional* change. In other words, we tend to be interested in variation that follows a trend, like secularization or the liberalization of attitudes about sexuality. Given that we are interested in how average opinions have changed in a single direction across our observation period, the linear assumption is theoretically justified. However, this does prevent us from saying anything about temporary changes, which can certainly be important for, e.g., electoral outcomes in specific elections.


```{r}
# Functions ----
getmod_lm_b2 <- function(df) {
  lm(y ~ cohort10t,
 	data = df)
}

getmod_lm_bw2 <- function(df) {
  lm(y ~ ns(year0, df = 2) * cohort10t,
 	data = df)
}

# get R2
get_r2 <- function(mod) {
  r2 <- r2(mod) %>% .[[1]] %>% as.numeric()
  return(r2)
}


all_cohorts_replace <- all_cohorts %>%
  mutate(cohort = rep(c("1920",
                    	"1940",
                    	"1960"),
                  	times = c(63, 63, 43)),
     	year0 = year-1938)

mod1 <- lm(opinion ~ cohort, all_cohorts_replace)
mod2 <- lm(opinion ~ ns(year0, df = 2) * cohort,
       	data = all_cohorts_replace)

first_mod_r2 <- round(get_r2(mod1)/get_r2(mod2), 2)


all_cohorts_period <- all_cohorts %>%
  mutate(
	cohort = rep(c("1920",
               	"1940",
               	"1960"),
             	times = c(63, 63, 43)),
	year0 = year - 1938,
	opinion = if_else(year < 1960,
                  	opinion,
                  	opinion + (year - 1938) * 0.01)
  )

mod3 <- lm(opinion ~ cohort, all_cohorts_period)
mod4 <- lm(opinion ~ ns(year0, df = 2) * cohort,
       	data = all_cohorts_period)

second_mod_r2 <- round(get_r2(mod3)/get_r2(mod4), 2)

all_cohorts_nodiff <- all_cohorts_same %>%
  mutate(cohort = rep(c("1920",
                    	"1940",
                    	"1960"),
                  	times = c(63, 63, 43))) %>%
  group_by(cohort) %>%
  mutate(year0 = year-min(year)+1,
     	opinion = opinion + year0 * 0.01) %>%
  ungroup()

mod5 <- lm(opinion ~ cohort, all_cohorts_nodiff)
mod6 <- lm(opinion ~ ns(year0, df = 2) * cohort,
       	data = all_cohorts_nodiff)

third_mod_r2 <- round(get_r2(mod5)/get_r2(mod6), 2)
```

Comparing these two models can help us quantify the relative importance of between-cohort and within-cohort change. The second model -- as a superset of the first -- will always account for more of the variance in the outcome. Therefore dividing the variance explained by the first model by that accounted for by the second one, we have a measure that represents the proportion of variance explained that is preserved when only *between-cohort differences* matter - i.e. when we do not allow within-cohort change. We call this proportion $\tau$:

$$ \tau = \frac{R^2_{\text{M1}}}{R^2_{\text{M2}}} $$

This measure may seem simple---perhaps too simple. But it captures the intuition behind comparing the models. Values of $\tau$ closer to one indicate that within-cohort changes add nothing to a model that includes only between cohort differences. For example, in the rather simple scenarios we discussed above, $\tau$ for the first case would be `r first_mod_r2` and in the second case would be `r second_mod_r2`. For the third -- admittedly extreme -- case, $\tau$ would be `r third_mod_r2`. Almost all variance explained is preserved in the first case when we take the effect of survey year out of the model. In the second case, we lose information, as expected, because the model does not allow within-cohort changes. In the third case, almost *none* of the variance explained is preserved when we do not consider *within-cohort changes*, as these explain almost all the change in aggregate opinion. This simple metric, then, is a useful way to differentiate the mechanisms that underpin the large-scale opinion change in repeated cross-sectional data.

### Data 

To compare mechanisms of change across different contexts and issues, we use the World Values Survey (WVS) [@inglehartWorldValuesSurveys2000]. The WVS, which began in 1981, is a large-scale effort to collect comparable data on beliefs and attitudes across multiple countries. For each country and each wave, the WVS collects high-quality, nationally-representative samples, and covers a wide range of questions from views on gender equality to socioeconomic indices. The survey, however, is not longitudinal, which means that we are unable to track any within-individual changes across time. However, it does allow us to examine trends in aggregate opinion across time for different countries. 

Previous work has used the WVS to examine different mechanisms of social change to great effect [@tormosRhythmModernizationHow2021; @tormosPatternsChangeJustifiability2023]. Our work builds on this literature in two ways. First, we build on conceptual debates that have focused primarily on the U.S. context and apply them cross-culturally. Second, we not only compare trends across countries, but also across different types of variables to examine whether mechanisms of social change vary along these two axes. 

Our method requires aggregate information in each country across a considerable time span. This is a challenge because not all countries feature in every wave of the WVS, and not all questions were asked in the times when we do have samples. For our analysis, we selected countries based on completeness---those for which we have the most measures over the longest period. This led us to select eight countries: Argentina (ARG), Australia (AUS), Canada (CAN), Japan (JPN), Mexico (MEX), South Africa (ZAF), Sweden (SWE), and the USA. 

This is not comprehensive or particularly diverse sample of countries. We are missing some of the world's most populous countries -- India and China -- and we do not have a majority-Muslim country. However, given that these mechanisms of social change have yet to be compared across different societies on many variables, an initial comparison -- albeit limited -- is valuable. 

We also selected variables for our analysis based on relevance and completeness. In terms of the former, we choose variables that reflect cultural attitudes that could plausibly change over time. This includes a wide range of items, from opinions about child-rearing to attitudes about the acceptability of euthanasia. We also select variables based on whether they have been asked in all the waves for the countries selected. After implementing both criteria we are left with 56 variables that cover a wide variety of issues, some mundane and some highly sensitive. The full list of items, alongside their respective questions and the abbreviations we use below, is available in the supplementary materials.

Given that we are interested in how sensitive (or not) these questions are, we fielded a multi-country survey to measure sensitivity. Though operationalizing sensitivity is difficult, we find the definition given in this special issue a useful starting point. As mentioned above, we follow Campbell and Mace (this issue) in defining sensitive topics as topics that are difficult to talk about. We took this definition and asked respondents to tell us how easy or difficult it would be to discuss our 56 survey questions from the WVS. 

Importantly, we are not interested in the respondents' own opinions on a given issue, but rather on how difficult they think it would be for the *majority of their compatriots* to talk about that question. Thus, we asked them: "how difficult would it be for the majority of people from you country to discuss the following question". We then provided them with a scale from 1 to 10, where 1 was labeled "not difficult at all" and 10 was labeled "extremely difficult". Each participant rated all 56 questions. This provides a plausible measure of how sensitive each issue is in each of the eight countries in our survey sample.

To field the surveys, we used the online platforms Prolific and CloudResearch. Both offer a high-quality pool of respondents across different countries [@peerTurkAlternativePlatforms2017]. Using both platforms, we were able to reach respondents from the eight countries that comprise our WVS sample. We translated all the questions to the main languages spoken in each country, and we gave participants the opportunity to choose their preferred language. Initially, our sample consisted of 808 individuals and, after excluding participants that had missed more than two attention checks, we had total sample of 802 respondents. Table 1 breaks down how this sample is distributed across the countries:


```{r}

usa_d <- read_csv("Data/usa_data_full.csv") %>% mutate(country = "USA")
# Keep only the real answers (no headers, pretests or rejections)
usa_d <- usa_d[-c(1:5, 16:17, 19) ,]

swe_d <- read_csv("Data/swe_data_full.csv") %>% mutate(country = "SWE")
swe_d <- swe_d[-c(1:2),]

zaf_d <- read_csv("Data/zaf_data_full.csv") %>% mutate(country = "ZAF")
zaf_d <- zaf_d[-c(1:2),]

mex_d <- read_csv("Data/mex_data_full.csv") %>% mutate(country = "MEX")
mex_d <- mex_d[-c(1:2),]

jpn_d <- read_csv("Data/jpn_data_full.csv") %>% mutate(country = "JPN")
jpn_d <- jpn_d[-c(1:2),]

can_d <- read_csv("Data/can_data_full.csv") %>% mutate(country = "CAN")
can_d <- can_d[-c(1:2),]

aus_d <- read_csv("Data/aus_data_full.csv") %>% mutate(country = "AUS")
aus_d <- aus_d[-c(1:2),]

arg_d <- read_csv("Data/arg_data_full.csv") %>% mutate(country = "ARG")
arg_d <- arg_d[-c(1:8,60),]


usa_ds <- usa_d %>%
  select(23:77,81)

swe_ds <- swe_d %>%
  select(23:77,81)

zaf_ds <- zaf_d %>%
  select(23:77,81)

mex_ds <- mex_d %>%
  select(23:77,81)

jpn_ds <- jpn_d %>%
  select(23:77,81)

can_ds <- can_d %>%
  select(23:77,81)

aus_ds <- aus_d %>%
  select(23:77,81)

arg_ds <- arg_d %>%
  select(22:76,81)

d_sens_complete <- rbind(usa_ds,
                     	swe_ds,
                     	zaf_ds,
                     	mex_ds,
                     	jpn_ds,
                     	can_ds,
                     	aus_ds,
                     	arg_ds)

d_sens_complete <- d_sens_complete %>%
  mutate(
	across(
  	.cols =1:55,
  	as.numeric
	)
  )

# If attention check succesful then 1, else 0
# Count attention checks
d_sens_complete <- d_sens_complete %>%
  mutate(ac1 = if_else(att1_1 == 2, 1, 0),
     	ac2 = if_else(att2_1 == 5, 1, 0),
     	ac3 = if_else(att3_1 == 1, 1, 0),
     	ac = ac1+ac2+ac3)
rm(usa_d,
   swe_d,
   zaf_d,
   mex_d,
   jpn_d,
   can_d,
   aus_d)

rm(usa_ds,
   swe_ds,
   zaf_ds,
   mex_ds,
   jpn_ds,
   can_ds,
   aus_ds)

# Keep only respondents with 2 or more attention checks
d_sens_complete <- d_sens_complete %>%
  filter(ac >=2)

d_sens_long <- d_sens_complete %>%
  select(country, everything()) %>%
  mutate(
	across(
  	.cols = 2:56,
  	as.numeric
	)
  ) %>%
  pivot_longer(
	2:56,
	names_to = "question",
	values_to = "difficulty"
  ) %>%
  mutate(
	question = str_remove(question, "_1$"),
	variable = case_when(
  	question == "imp_family" ~ "important_family",
  	question == "imp_friends" ~ "important_friends",
  	question == "imp_leisure" ~ "important_leisure",
  	question == "imp_pol" ~ "important_politics",
  	question == "imp_wrk" ~ "important_work",
  	question == "imp_rel" ~ "important_religion",
  	question == "child_qualities" ~ "child_independence",
  	question == "neigh_race" ~ "neigh_diff_race",
  	question == "neigh_drink" ~ "neigh_drink",
  	question == "neigh_imm" ~ "neigh_imm",
  	question == "neigh_aids" ~ "neigh_aids",
  	question == "neigh_drug" ~ "neigh_drugs",
  	question == "neigh_gay" ~ "neigh_gay",
  	question == "trust" ~ "trust_people",
  	question == "life_satisf" ~ "life_satisf",
  	question == "choice_control" ~ "choice_control",
  	question == "scarce_jobs_women" ~ "jobs_men_over_women",
  	question == "scarce_jobs_imm" ~ "jobs_national_over_foreign",
  	question == "housewife_fulfill" ~ "housewife_fulfilling",
  	question == "left_right" ~ "politics_scale",
  	question == "equal_incomes" ~  "income_eq",
  	question == "private_public_busi" ~ "pvt_state_owned",
  	question == "gvt_responsibility" ~ "gvt_responsibility",
  	question == "comp_good_bad" ~ "competition_good_evil",
  	question == "conf_church" ~ "confidence_churches",
  	question == "conf_armed" ~ "confidence_armed_forces",
  	question == "conf_press" ~ "confidence_press",
  	question == "conf_unions" ~ "confidence_unions",
  	question == "conf_police" ~ "confidence_police",
  	question == "conf_parliament" ~ "confidence_parliament",
  	question == "conf_civil" ~ "confidence_civil",
  	question == "conf_tv" ~ "confidence_tv",
  	question == "conf_gvt" ~ "confidence_government",
  	question == "conf_pol_prt" ~ "confidence_political_parties",
  	question == "conf_companies" ~ "confidence_major_companies",
  	question == "conf_courts" ~ "confidence_justice_courts",
  	question == "rel_services" ~ "attend_relig",
  	question == "religiosity" ~ "religious_person",
  	question == "belief_god" ~ "believe_god",
  	question == "belief_hell" ~ "believe_hell",
  	question == "imp_god" ~ "important_god",
  	question == "just_gvt_benefits" ~  "just_gvt_benefits",
  	question == "just_public_trans" ~ "just_fare_public_trans",
  	question == "just_cheat_tax" ~ "just_cheat_taxes",
  	question == "just_bribe" ~ "just_bribe",
  	question == "just_homo" ~ "just_homosexuality",
  	question == "just_prst" ~ "just_prostitution",
  	question == "just_abort" ~ "just_abortion",
  	question == "just_divorce" ~ "just_divorce",
  	question == "just_euth" ~ "just_euthanasia",
  	question == "just_suicide" ~ "just_suicide",
  	question == "proud" ~ "proud_nationality",
  	TRUE ~ question),
  )


d_sens_complete %>%
  group_by(country) %>%
  summarise(`Number of Respondents` = n()) %>%
	kableExtra::kable(format = "latex",
	align = "c",
	booktabs = TRUE,
	longtable = TRUE,
	linesep = "", 
	caption = "Sample size by country for the survey examining difficulty to discuss the WVS items."
	) %>%
  kableExtra::kable_styling(
  	latex_options = c("striped", "repeat_header", "hold_position"),
  	stripe_color = "gray!15",
  	font_size = 6
	) 

```


We do not claim that this sample is representative of the population of any of those countries. However, we believe that these data provide a principled measure of how sensitive certain issues are perceived in each country. The fact that we prompted participants to think about their second-order beliefs -- to think about what most of their compatriots think -- helps in trying to bypass individual idiosyncrasies and to get a adequate measure of country-level perceptions of sensitivity. This is reflected in the fact that the overall patterns in our data are plausible. Table 2, for example, shows the median score for the three issues that respondents, in each country, rated as the most difficult to discuss (we provide full descriptive results of the survey data in the supplementary materials):  


```{r}
# p_usa <- d_sens_long %>%
#   filter(country == "USA") %>%
#   group_by(variable) %>%
#   summarise(avg = median(difficulty)) %>%
#   ungroup() %>%
#   arrange(desc(avg)) %>%
#   slice(1:5) %>%
#   ggplot(
# 	aes(x = fct_reorder(variable, avg),
#     	y = avg)
#   ) +
#   geom_point(pch=23) +
#   coord_flip() +
#   labs(title = "USA",
#    	y = "Median Difficulty",
#    	x = "") +
#   theme(text = element_text(size = 6))
#
# p_swe <- d_sens_long %>%
#   filter(country == "SWE") %>%
#   group_by(variable) %>%
#   summarise(avg = median(difficulty)) %>%
#   ungroup() %>%
#   arrange(desc(avg)) %>%
#   slice(1:5) %>%
#   ggplot(
# 	aes(x = fct_reorder(variable, avg),
#     	y = avg)
#   ) +
#   geom_point(pch=23) +
#   coord_flip() +
#   labs(title = "SWE",
#    	y = "Median Difficulty",
#    	x = "") +
#   theme(text = element_text(size = 6))
#
# p_zaf <- d_sens_long %>%
#   filter(country == "ZAF") %>%
#   group_by(variable) %>%
#   summarise(avg = median(difficulty)) %>%
#   ungroup() %>%
#   arrange(desc(avg)) %>%
#   slice(1:5) %>%
#   ggplot(
# 	aes(x = fct_reorder(variable, avg),
#     	y = avg)
#   ) +
#   geom_point(pch=23) +
#   coord_flip() +
#   labs(title = "ZAF",
#    	y = "Median Difficulty",
#    	x = "") +
#   theme(text = element_text(size = 6))
#
# p_mex <- d_sens_long %>%
#   filter(country == "MEX") %>%
#   group_by(variable) %>%
#   summarise(avg = median(difficulty)) %>%
#   ungroup() %>%
#   arrange(desc(avg)) %>%
#   slice(1:5) %>%
#   ggplot(
# 	aes(x = fct_reorder(variable, avg),
#     	y = avg)
#   ) +
#   geom_point(pch=23) +
#   coord_flip() +
#   labs(title = "MEX",
#    	y = "Median Difficulty",
#    	x = "") +
#   theme(text = element_text(size = 6))
#
# p_jpn <- d_sens_long %>%
#   filter(country == "JPN") %>%
#   group_by(variable) %>%
#   summarise(avg = median(difficulty)) %>%
#   ungroup() %>%
#   arrange(desc(avg)) %>%
#   slice(1:5) %>%
#   ggplot(
# 	aes(x = fct_reorder(variable, avg),
#     	y = avg)
#   ) +
#   geom_point(pch=23) +
#   coord_flip() +
#   labs(title = "JPN",
#    	y = "Median Difficulty",
#    	x = "") +
#   theme(text = element_text(size = 6))
#
# p_can <- d_sens_long %>%
#   filter(country == "CAN") %>%
#   group_by(variable) %>%
#   summarise(avg = median(difficulty)) %>%
#   ungroup() %>%
#   arrange(desc(avg)) %>%
#   slice(1:5) %>%
#   ggplot(
# 	aes(x = fct_reorder(variable, avg),
#     	y = avg)
#   ) +
#   geom_point(pch=23) +
#   coord_flip() +
#   labs(title = "CAN",
#    	y = "Median Difficulty",
#    	x = "") +
#   theme(text = element_text(size = 6))
#
# p_arg <- d_sens_long %>%
#   filter(country == "ARG") %>%
#   group_by(variable) %>%
#   summarise(avg = median(difficulty)) %>%
#   ungroup() %>%
#   arrange(desc(avg)) %>%
#   slice(1:5) %>%
#   ggplot(
# 	aes(x = fct_reorder(variable, avg),
#     	y = avg)
#   ) +
#   geom_point(pch=23) +
#   coord_flip() +
#   labs(title = "ARG",
#    	y = "Median Difficulty",
#    	x = "") +
#   theme(text = element_text(size = 6))
#
# p_aus <- d_sens_long %>%
#   filter(country == "AUS" &
#        	!str_detect(variable,"att")) %>%
#   group_by(variable) %>%
#   summarise(avg = median(difficulty)) %>%
#   ungroup() %>%
#   arrange(desc(avg)) %>%
#   slice(1:5) %>%
#   ggplot(
# 	aes(x = fct_reorder(variable, avg),
#     	y = avg)
#   ) +
#   geom_point(pch=23) +
#   coord_flip() +
#   labs(title = "AUS",
#    	y = "Median Difficulty",
#    	x = "") +
#   theme(text = element_text(size = 6))
#
# p_usa + p_swe + p_zaf + p_mex + p_jpn + p_can + p_arg + p_aus

d_sens_long %>%
  group_by(country,
       	variable) %>%
  summarise(`Median Difficulty` = median(difficulty, na.rm = T)) %>%
  ungroup() %>%
  group_by(country) %>%
  arrange(desc(`Median Difficulty`)) %>%
  slice(1:3) %>%
  ungroup() %>%
  mutate(
	Variable = case_when(
  	variable == "confidence_government" ~ "Confidence in government",
  	variable == "just_bribe" ~ "Justifiability of bribes",
  	variable == "just_euthanasia" ~ "Justifiability of euthanasia",
  	variable == "just_suicide" ~ "Justifiability of suicide",
  	variable == "just_abortion" ~ "Justifiability of abortion",
  	variable == "neigh_gay" ~ "Having a gay neighbor",
  	variable == "jobs_men_over_women" ~ "Jobs for men over women",
  	variable == "just_homosexuality" ~ "Justifiability of homosexuality",
  	variable == "jobs_national_over_foreign" ~ "Jobs for national over foreigners",
  	variable == "neigh_diff_race" ~ "Having a neighbor of a different race"
	)
  ) %>%
  select(-c(country, variable)) %>%
  kbl(., booktabs = T, 
      caption = "The three issues rated as the most sensitive in each country") %>%
  kable_styling(
	latex_options = c("striped", "hold_position"),
	stripe_color = "gray!15",
	font_size = 6
  ) %>%
  pack_rows("ARG", 1, 3) %>%
  pack_rows("AUS", 4, 6) %>%
  pack_rows("CAN", 7, 9) %>%
  pack_rows("JPN", 10, 12) %>%
  pack_rows("MEX", 13, 15) %>%
  pack_rows("SWE", 16, 18) %>%
  pack_rows("USA", 19, 21) %>%
  pack_rows("ZAF", 22, 24)

```


Unsurprisingly, the questions about the justifiability of abortion, euthanasia, and suicide feature prominently in most countries, showing cross-national similarities in perceptions of sensitivity. However, we also pick up on some context-specific patterns: while issues of corruption and government credibility are perceived as difficult to discuss in Argentina, question about race and immigration are seen as particularly sensitive in Sweden. This resonates with both the recent past of these countries and their current circumstances. The full descriptive summary of the data is provided in the supplementary materials, but overall the results seem to capture intuitive general trends while allowing for context-specific variation. While capturing the notion of sensitivity is challenging, we believe our approach measures this concept reasonably well.

### Analysis 

We begin our analyses by examining the relationship between $\tau$ and the total amount of change each variable exhibits. For each variable, we calculate $\tau$, as defined above, and then plot it against how much it has changed across the decades of observation (in standard deviations calculated by pooling the first and last waves). We then fit two regression models to explore whether the perceived sensitivity of an issue predicts how much it changes and the proportion of linear change attributable to between-cohort differences ($\tau$). We use the median because it is less sensitive to extreme values, but our results show similar patterns when we use the mean sensitivity for each variable. In turn, the dependent variables of interest are, respectively, the absolute change exhibited by each variable in each country and the calculated $\tau$. In both models, we allow for varying intercepts and slopes across countries to account for cultural variation -- and similarities -- between the different contexts.  

```{r}
# Data importation & cleaning ----
load("Data/wvs_timeseries.rdata")
d <- data1
rm(data1)

# Read in the data
d_short <- d %>%
  select(S002VS,
     	COUNTRY_ALPHA,
     	S020,
     	S003,
     	X001,
     	X003,
     	X007,
     	A001:A006,
     	A029:A042,
     	A124_02,
     	A124_03,
     	A124_06:A124_09,
     	A165,
     	A170,
     	A173,
     	C001,
     	C002,
     	D057,
     	E033,
     	E035:E037,
     	E039,
     	E069_01:E069_02,
     	E069_04:E069_08,
     	E069_10:E069_13,
     	E069_17,
     	F028,
     	F034,
     	F050,
     	F053,
     	F063,
     	F114A,
     	F115:F123,
     	G006) %>%
  rename(wave = S002VS,
     	country = COUNTRY_ALPHA,
     	country_code = S003,
     	year_survey = S020,
     	sex = X001,
     	age = X003,
    	 
     	# Importance battery: 1 means very important, 4 means not at all important
     	important_family = A001,
     	important_friends = A002,
     	important_leisure = A003,
     	important_poltics = A004,
     	important_work = A005,
     	important_religion = A006,
    	 
     	# Important qualities in children: 1 means mentioned, 0 means not mentioned
     	child_independence = A029,
     	child_hard_work = A030,
     	child_feeling_responsibility = A032,
     	child_imagination = A034,
     	child_tolerance = A035,
     	child_thrift = A038,
     	child_determination = A039,
     	child_religion = A040,
     	child_unselfish = A041,
     	child_obedience = A042,
    	 
     	# People you would not like to have as neighbours: 1 mentioned, 0 means not mentioned
     	neigh_diff_race = A124_02,
     	neigh_drink = A124_03,
     	neigh_imm = A124_06,
     	neigh_aids = A124_07,
     	neigh_drugs = A124_08,
     	neigh_gay = A124_09,
    	 
     	# Most people can be trusted: (1) Most people, (2) you can never be too careful
     	trust_people = A165,
    	 
     	# (10) means more satisfied
     	life_satisf = A170,
    	 
     	# Control over life's choices: (10) feels most in control.
     	choice_control = A173,
    	 
     	# Men should be given jobs over women: (1) Agree, (2) Disagree, (3) Neither A nor D
     	jobs_men_over_women = C001,
    	 
     	# Nationals should be given jobs over foreigners: (1) Agree, (2) Disagree, (3) Neither A nor D
     	jobs_national_over_foreign = C002,
    	 
     	# Likert: (1) Strongly Agree -- (4) Strongly Disagree
     	housewife_fulfilling = D057,
    	 
     	# Left/Right placement: (1) Left -- (10) Right
     	politics_scale = E033,
    	 
     	# Importance of income equality
     	income_eq = E035,
    	 
     	# Business should be public/private owned: (1) Private -- (10) Public
     	pvt_state_owned = E036,
    	 
     	# Government should take responsibility vs individual responsibility: (1) Gvt -- (10) Individual
     	gvt_responsibility = E037,
    	 
     	# Competition good or harmful: (1) Good --- (10) harmful
     	competition_good_evil = E039,
    	 
     	# Confidence battery: (1) A great deal; (2) Quite a lot; (3) Not very much; (4) None at all
     	confidence_churches = E069_01,
     	confidence_armed_forces = E069_02,
     	confidence_press = E069_04,
     	confidence_unions = E069_05,
     	confidence_police = E069_06,
     	confidence_parliament = E069_07,
     	confidence_civil = E069_08,
     	confidence_television = E069_10,
     	confidence_governement = E069_11,
     	confidence_political_party = E069_12,
     	confidence_major_companies = E069_13,
     	confidence_justice_courts = E069_17,
    	 
     	# How often attends religious services?
     	# (1) More than once a week -- (8) Never
     	attend_relig = F028,
    	 
     	# (1) Religious; (2) not religious; (3) atheist
     	religious_person = F034,
    	 
     	# Believes (1), does not believe (0)
     	believe_god = F050,
     	believe_hell = F053,
    	 
     	# (1) Not at all important -- (10) very important
     	important_god = F063,
     	just_gvt_benefits = F114A,
     	just_fare_public_trans = F115,
     	just_cheat_taxes = F116,
     	just_bribe = F117,
     	just_homosexuality = F118,
     	just_prostitution = F119,
     	just_abortion = F120,
     	just_divorce = F121,
     	just_euthanasia = F122,
     	just_suicide = F123,
     	proud_nationality = G006,
     	marital_status = X007)



# Zap labels
d_short <- haven::zap_labels(d_short)

d_waves <- d_short %>%
  pivot_longer(important_family:proud_nationality,
           	names_to = "variable",
           	values_to = "y") %>%
  mutate(age01 = (age - 25)/(64 - 25),
     	birthyear = year_survey - age,
     	year0 = year_survey - 1981,
     	cohort5 = floor(birthyear/5) * 5,
     	cohort10 = floor(birthyear/10) * 10) %>%
  drop_na(c(y, birthyear)) %>%
  group_by(country, variable) %>%
  mutate(start = min(year_survey),
     	end = max(year_survey),
     	span = end - start,
     	waves = n_distinct(wave),
     	obs = n()) %>%
  ungroup()

# Select sensitivity measures for country-year
sens_df <- d_waves %>%
  filter(waves >= 4 & span >= 30 & age > 24) %>%
  group_by(country,
       	variable) %>%
  mutate(num_people = n()) %>%
  ungroup() %>%
  group_by(country,
       	variable,
       	y) %>%
  mutate(num_answer = n(),
     	percentage_answer = num_answer/num_people) %>%
  slice(1) %>%
  ungroup() %>%
  filter(y == -2) %>%
  select(country, variable, percentage_answer)

# Previous analyses

d_short_clean <- d_short %>%
  mutate(
	across(
  	.cols = 5:68,
  	~ ifelse(
    	. < 0,
    	NA_real_,
    	.
  	))
  )

# Create the variables for manipulation

d_waves_clean <- d_short_clean %>%
  pivot_longer(important_family:proud_nationality,
           	names_to = "variable",
           	values_to = "y") %>%
  mutate(age01 = (age - 25)/(64 - 25),
     	birthyear = year_survey - age,
     	year0 = year_survey - 1981,
     	cohort5 = floor(birthyear/5) * 5,
     	cohort10 = floor(birthyear/10) * 10) %>%
  drop_na(c(y, birthyear)) %>%
  group_by(country, variable) %>%
  mutate(start = min(year_survey),
     	end = max(year_survey),
     	span = end - start,
     	waves = n_distinct(wave),
     	obs = n()) %>%
  ungroup()

# Filter dataset appropriately
d30 <- d_waves_clean %>%
  filter(waves >= 4 & span >= 30 & age > 24)

d30 <- d30 %>%
  mutate(cohort10t = case_when(
	cohort10 < 1920 ~ 1920,
	cohort10 > 1980 ~ 1980,
	TRUE ~ cohort10
  ))

# Functions
getmod_lm_b2 <- function(df) {
  lm(y ~ cohort10t,
 	data = df)
}

getmod_lm_bw2 <- function(df) {
  lm(y ~ ns(year0, df = 1) * cohort10t,    	# spline (or linear; check current setting)
 	data = df)                            	# df=2 means one bend; df=1 means line
}

# get R2
get_r2 <- function(mod) {
  r2 <- r2(mod) %>% .[[1]] %>% as.numeric()
  return(r2)
}

# Analysis

# Nest dataset
d30_nest <- d30 %>%
  group_by(country, variable) %>%
  nest()

# Carry out analyses
results <- d30_nest %>%
  mutate(mb = map(data, getmod_lm_b2),
     	mbw = map(data, getmod_lm_bw2),
     	rb = map_dbl(mb, get_r2),
     	rbw = map_dbl(mbw, get_r2),
     	pbetween = rb/rbw) %>%
  select(country, variable, rb, rbw, pbetween)

# CHANGING X-AXIS TO ABSOLUTE CHANGE

# Find variables with meaningful changes from first to last wave
d_change <- d_waves_clean %>%
  filter(waves >= 4 & span >= 30 & age > 24) %>%
  mutate(first_obs = min(year0),
     	last_obs = max(year0),
     	.by = c(country, variable)) %>%
  filter(year0 == first_obs | year0 == last_obs) %>%
  mutate(post = if_else(year0 == last_obs, 1L, 0L)) %>%
  mutate(ymean = mean(y),
     	.by = c(country, variable, post)) %>%
  mutate(ysd = sd(y),
     	.by = c(country, variable)) %>%
  group_by(country, variable, post) %>%
  select(ymean, ysd) %>%
  slice_head(n = 1) %>%
  ungroup() %>%
  pivot_wider(names_from = post,
          	values_from = ymean,
          	names_prefix = "mean") %>%
  mutate(change = (mean1 - mean0) / ysd,
     	abschange = abs(change))

# attach to results
results <- left_join(results,
                 	select(d_change, country, variable, abschange))


```

## Results

We begin by calculating $\tau$ for all variables across each country. Figure 4 displays the relationship between $\tau$ on the y-axis and absolute change of the variable across the recorded time span on the x-axis:


```{r, fig.cap="Relationship between the variance explained preserved when linear within-cohort changes are assumed to be zero and absolute change -- in standard deviations - between the first and last wave."}
# Results
p4 <- ggplot(results,
   	aes(x = abschange,
       	y = pbetween,
       	label = variable,
       	color = variable)) +
  geom_point(alpha = 0.3) +
  geom_text_repel(
	data = results %>%
  	filter(abschange >= .8),
	size = 2.5,
	box.padding = 0.75
  ) +
  facet_wrap(~country) +
  theme(legend.position = "none") +
  labs(
	title = "",
	y = expression(tau),
	x = "Standardized change, 1981-2020"
  )

p4 

ggsave("~/Documents/sensitive_change/tau_change.png", p4, dpi = 500)
```


On the y-axis, we have $\tau$, which can be interpreted variance explained that is preserved when linear *within-cohort changes* are assumed to be zero. On the x-axis, we have the absolute change - in standard deviations - between the first and last wave. Therefore, in the upper right quadrant of each plot, we should see variables that have changed a lot and whose mean change can be well predicted by simple cohort replacement. In the lower right quadrant, we should see variables that have also exhibited a lot of change but whose variations are mostly accounted for by *within-cohort change*.

The first striking result is that most variables do not change much; most variables hover around the left-hand side of the x-axis. This is most evident in countries like Mexico and South Africa. In the plot, we label the variables that have displayed a directional change higher than 0.8 standard deviations. 

Our results also seem to capture certain historical changes that we would expect given the time when the surveys were administered. For instance, changes in confidence in justice courts in Argentina coincide with the famous trials of the military dictatorship and the variation in confidence in the armed forces in Japan runs parallel with a restructuring of that institution. 

Another important pattern that emerges is that, consistent with previous work on cultural change [@kileyMeasuringStabilityChange2020; @tormosRhythmModernizationHow2021], the variable that seems to display consistently large change across countries is the justifiability of homosexuality. Notice that this variable tends to be on upper-right quadrant, which suggests that this change in mean over time is the result of *between-cohort differences*, with *within-cohort change* contributing a relatively small proportion of the change. In fact, at first glance, we see that this seems to be a common pattern for sensitive issues such as the justifiability of abortion, euthanasia, and divorce. When they do exhibit considerable change, most of the variation is explained by cohort differences. The variables that exhibit change through *within-cohort change* are mostly related to confidence in institutions and childrearing. Thus, there seems to be a pattern in how change in different variables reflects different change mechanisms and it seems to be related to issue sensitivity.

We test the possible relevance of sensitivity directly in two ways. First, we examine whether the sensitivity of an issue is predictive of how much change it has undergone. Second, we analyze whether sensitivity predicts a variable's $\tau$.

To see if sensitive issues display different rates of overall change, we fit a linear regression model where the outcome is overall change and the main predictor is an issue's median sensitivity in each country. Given that the outcome variable is truncated at zero -- a variable cannot display less than no change -- we use the lognormal link. As mentioned above, the model includes varying intercepts and slopes at the level of country. Panel A in Figure 5 shows the posterior distribution of the population-level coefficient for sensitivity in this model, in the log scale (a formal definition of the model, detailed results, and assessments of fit are included in the supplementary materials). The distribution is centered around 0.15, and a considerable amount of the mass lies below 0. The uncertainty in this posterior distribution suggests that -- in our data -- there is not a strong relationship between an issue's perceived sensitivity and the amount of change.


```{r, fig.cap="a) posterior estimate of the coefficient for sensitivity for the model regressing absolute change on sensitivity; b) bimodality of tau; c) posterior estimate of the coefficient for sensitivity for the model regressing the probability of a value being drawn from the higher beta distribution on sensitivity."}
# Our child qualities question maps on to a few in the WVS
# Let's create those
d_sens_complete <- d_sens_complete %>%
  mutate(
	child_hard_work = child_qualities_1,
	child_feeling_responsibility = child_qualities_1,
	child_imagination = child_qualities_1,
	child_tolerance = child_qualities_1,
	child_thrift = child_qualities_1,
	child_determination = child_qualities_1,
	child_religion = child_qualities_1,
	child_unselfish = child_qualities_1,
	child_obedience = child_qualities_1
  )

d_sens_long <- d_sens_complete %>%
  select(country, imp_family_1:just_suicide_1, proud_1, child_hard_work:child_obedience) %>%
  mutate(
	across(
  	.cols =2:62,
  	as.numeric
	)
  ) %>%
  pivot_longer(
	2:62,
	names_to = "question",
	values_to = "difficulty"
  ) %>%
  mutate(
	question = str_remove(question, "_1$"),
	variable = case_when(
  	question == "imp_family" ~ "important_family",
  	question == "imp_friends" ~ "important_friends",
  	question == "imp_leisure" ~ "important_leisure",
  	question == "imp_pol" ~ "important_politics",
  	question == "imp_wrk" ~ "important_work",
  	question == "imp_rel" ~ "important_religion",
  	question == "child_qualities" ~ "child_independence",
  	question == "neigh_race" ~ "neigh_diff_race",
  	question == "neigh_drink" ~ "neigh_drink",
  	question == "neigh_imm" ~ "neigh_imm",
  	question == "neigh_aids" ~ "neigh_aids",
  	question == "neigh_drug" ~ "neigh_drugs",
  	question == "neigh_gay" ~ "neigh_gay",
  	question == "trust" ~ "trust_people",
  	question == "life_satisf" ~ "life_satisf",
  	question == "choice_control" ~ "choice_control",
  	question == "scarce_jobs_women" ~ "jobs_men_over_women",
  	question == "scarce_jobs_imm" ~ "jobs_national_over_foreign",
  	question == "housewife_fulfill" ~ "housewife_fulfilling",
  	question == "left_right" ~ "politics_scale",
  	question == "equal_incomes" ~  "income_eq",
  	question == "private_public_busi" ~ "pvt_state_owned",
  	question == "gvt_responsibility" ~ "gvt_responsibility",
  	question == "comp_good_bad" ~ "competition_good_evil",
  	question == "conf_church" ~ "confidence_churches",
  	question == "conf_armed" ~ "confidence_armed_forces",
  	question == "conf_press" ~ "confidence_press",
  	question == "conf_unions" ~ "confidence_unions",
  	question == "conf_police" ~ "confidence_police",
  	question == "conf_parliament" ~ "confidence_parliament",
  	question == "conf_civil" ~ "confidence_civil",
  	question == "conf_tv" ~ "confidence_tv",
  	question == "conf_gvt" ~ "confidence_government",
  	question == "conf_pol_prt" ~ "confidence_political_parties",
  	question == "conf_companies" ~ "confidence_major_companies",
  	question == "conf_courts" ~ "confidence_justice_courts",
  	question == "rel_services" ~ "attend_relig",
  	question == "religiosity" ~ "religious_person",
  	question == "belief_god" ~ "believe_god",
  	question == "belief_hell" ~ "believe_hell",
  	question == "imp_god" ~ "important_god",
  	question == "just_gvt_benefits" ~  "just_gvt_benefits",
  	question == "just_public_trans" ~ "just_fare_public_trans",
  	question == "just_cheat_tax" ~ "just_cheat_taxes",
  	question == "just_bribe" ~ "just_bribe",
  	question == "just_homo" ~ "just_homosexuality",
  	question == "just_prst" ~ "just_prostitution",
  	question == "just_abort" ~ "just_abortion",
  	question == "just_divorce" ~ "just_divorce",
  	question == "just_euth" ~ "just_euthanasia",
  	question == "just_suicide" ~ "just_suicide",
  	question == "proud" ~ "proud_nationality",
  	TRUE ~ question),
  )

# Center the results
d_sens_long <- d_sens_long %>%
  group_by(
	country
  ) %>%
  mutate(
	sens_ctd = scale(difficulty)[,1]
  ) %>%  
  ungroup()

# Now calculate the median and mean
summary_sens <- d_sens_long %>%
  group_by(country,
       	variable) %>%
  summarise(
	avg = mean(sens_ctd, na.rm = T),
	med = median(sens_ctd, na.rm = T)
  ) %>%
  ungroup()

results_sens <- results %>%
  left_join(., summary_sens, by = c("country", "variable"))

saveRDS(results_sens,
    	"data_for_analysis.rds")

set.seed(43543)

# output1 <- capture.output(mod1 <- brms::brm(abschange ~ (med | country) + med,
#               	data = results_sens,
#               	family = "lognormal"))
# 
# saveRDS(mod1, "model_abschange.rds")

mod1 <- read_rds("model_abschange.rds")

# mix <- mixture(Beta, Beta)
# 
# output2 <- capture.output(mod2 <- brm(bf(pbetween ~ 1,
#            	theta2 ~ (med | country) + med),
#         	results_sens,
#         	family = mix,
#         	chains = 4,
#         	iter = 2000))
# 
# saveRDS(mod2, "model_beta.rds")

mod2 <- read_rds("model_beta.rds")

# output3 <- capture.output(mod3 <- brm(pbetween ~ (med | country) + med,
#         	results_sens,
#         	chains = 4,
#         	iter = 2000))
# 
# saveRDS(mod3, "model_linear.rds")

mod3 <- read_rds("model_linear.rds")

prds_mod1 <- prepare_predictions(mod1)["dpars"]

panel_a <- data.frame(prds_mod1$dpars$mu$fe$b) %>%
  ggplot(
	aes(x = b_med)
  ) +
  geom_density(fill = "gray80",
           	alpha = 0.8) +
  geom_vline(aes(xintercept = 0),
         	linetype = "dashed",
         	col="darkred") +
  labs(y = "",
   	x = "Estimate of Sensitivity (log scale)",
   	title = "(A)") +
  theme(text = element_text(size = 8))

panel_b <- results_sens %>%
  ggplot(aes(x = pbetween)) +
  geom_density(fill = "gray80",
           	alpha = 0.8) +
  labs(y = "",
   	x = "Distribution of Tau",
   	title = "(B)") +
  theme(text = element_text(size = 8))

prds_mod2 <- prepare_predictions(mod2)["dpars"]

panel_c <- data.frame(prds_mod2$dpars$theta2$fe$b) %>%
  ggplot(
	aes(x = b_theta2_med)
  ) +
  geom_density(fill = "gray80",
           	alpha = 0.8) +
  geom_vline(aes(xintercept = 0),
         	linetype = "dashed",
         	col="darkred") +
  labs(y = "",
   	x = "Estimate of Sensitivity (log-odds scale)",
   	title = "(C)") +
  theme(text = element_text(size = 8))

p5 <- panel_a + panel_b + panel_c

p5
ggsave("~/Documents/sensitive_change/posterior_distributions.png", p5, dpi = 500)
```

In models with varying slopes and intercepts, it is difficult to interpret population-level effects, so it is more intuitive to plot model-implied predictions. Figure 6 shows these predictions with the x-axis representing centered sensitivity scales ranging from -1.5 standard deviations below the mean to 1.5 standard deviations above. The lines represent the mean prediction for each value of sensitivity. We notice that, while the relationship appears to be positive in countries like Argentina and South Africa, it is flat in the rest of our sample. Thus, we find no compelling evidence to suggest a specific relationship between an issue's sensitivity and the amount of aggregate change and, therefore, it is not possible to draw any strong conclusions.

```{r, fig.cap="Model-implied average predictions for the model regressing absolute change on sensitivity"}
newdata <- expand_grid(
  country = unique(results_sens$country),
  med = seq(from =-1.5, to= 1.5, by=0.05)
)

newdata$preds <- predict(mod1, newdata)

p6 <- newdata %>%
  ggplot(aes(x = med,
         	y = preds[,1]),
     	fill = country)  +
  geom_line(linetype = "dashed")  +
  geom_point(
	data = results_sens,
	aes(x = med,
    	y = abschange),
	pch = 23
  ) +
  facet_wrap(~country) +
  labs(
	title = "",
	y = "Abs. Change",
	x = "Centered Sensitivity"
  )
p6 
ggsave("~/Documents/sensitive_change/change_sensitivity.png", p6, dpi = 500)
```

To model $\tau$ as a function of sensitivity, it is necessary to make a few additional adjustments. First, given that $\tau$ is a proportion, it is bounded between 0 and 1. Second, in our data, $\tau$ exhibits a clear bimodality. We address this by fitting a finite-mixture model where we consider $\tau$ as produced by two different beta distributions, themselves bounded between 0 and 1. Panel B of Figure 5 shows the bimodal distribution of $\tau$, which we are going to model as two beta distributions.

Within the model, we also regress the probability of an issue belonging to the distribution with higher $\tau$ on that issue's perceived sensitivity. In other words, we ask the question: does the perceived sensitivity of an issue tell us whether it is more likely to have emerged from the beta distribution with higher average $\tau$? If this is the case, then a higher sensitivity should be associated with a larger proportion of change explained solely by *between-cohort differences*. As above, in the regression component, we include varying intercepts and slopes at the level of country. It is worth noting that we also perform this analysis using Gaussian linear regression and the results are substantially the same. We include those analyses in the supplementary materials.

Panel C in Figure 5 displays the posterior distribution for the population-level coefficient for the effect of sensitivity on the probability of belonging to the distribution with higher $\tau$ (we include a formal definition of the model, detailed results, and assessments of fit are included in the supplementary materials). The coefficient is in the log-odds scale, but it is readily apparent that the majority of its mass lies above 0. The model suggests then that as an issue's sensitivity increases, so does the probability that it belongs to the distribution with higher average $\tau$.

However, results in the log-odds scales are notoriously difficult to interpret. Given the overall complexity of the model, it is better to examine its predictions to understand the implications of the results. Figure 7 displays the model's predicted outcomes, where the lines represent the average prediction at each value of sensitivity. We notice that the slopes consistently exhibit a slight, positive relationship. There is a small amount of variation between countries; for example, while a one standard deviation increase in sensitivity predicts an increase in $tau$ of 0.158 in Mexico, it predicts and increase of 0.147 in Sweden. Despite this variation, the evidence for a positive relationship is consistent across countries. Thus, although the effect of sensitivity is not large, our model suggests that -- on average -- we should expect more sensitive issues to change more through *between-cohort differences* rather than via *within-cohort changes*.    


```{r, fig.cap="Model-implied average predictions for the model regressing tau on sensitivity"}
newdata <- expand_grid(
  country = unique(results_sens$country),
  med = seq(from =-1.5, to= 1.5, by=0.05)
)

newdata$preds <- predict(mod2, newdata)

p7 <- newdata %>%
  ggplot(aes(x = med,
         	y = preds[,1]),
     	fill = country) +
  # geom_ribbon(aes(
  #   ymin = preds[,3],
  #   ymax = preds[,4]
  # ),
  # col = "gray80",
  # alpha = 0.1) +
  geom_line(linetype = "dashed") +
  geom_point(
	data = results_sens,
	aes(x = med,
    	y = pbetween),
	pch = 23
  ) +
  facet_wrap(~country) +
  labs(
	title = "",
	y = expression(tau),
	x = "Centered Sensitivity"
  )

p7 
ggsave("~/Documents/sensitive_change/senstivity_tau.png", p7, dpi = 500)
```


## Discussion and Conclusion

In this paper, we expanded on previous research on cultural change by investigating mechanisms that underlie processes of large-scale cultural change. Our investigation led to several relevant findings.

First, we found that most beliefs do not exhibit a large degree of linear change, even over four decades. As previous research has shown, across most countries, attitudes towards homosexuality do show considerable change [@tormosRhythmModernizationHow2021]. A few other variables show large changes as well but they vary from country to country without a clear pattern.

Second, we found that a considerable proportion of linear change on many issues can be approximated well by a simple model that assumes cohort succession is the only mechanism that can produce linear belief changes over time. The justifiability of homosexuality is one of these variables, indicating that the linear change in beliefs on this issue can mostly be mostly attributed to between-cohort differences. In some countries, we see a similar pattern for other sensitive issues like attitudes around divorce and euthanasia [@tormosPatternsChangeJustifiability2023]. 

Third, our models captured important historical contingencies that produced major within-cohort changes, such as the restructuring of the armed forces in Japan or the trials of the military dictatorship in Argentina. This provides some additional confidence that our models are not stacking the deck in favor of cohort replacement mechanisms.

Lastly---and most important---we found that, although beliefs about sensitive issues do not change more than beliefs about mundane issues, they do seem to change more via cohort replacement. This provides an interesting window into one issue-specific mechanism that might influence how cultural change happens. We cannot know whether this pattern is the result of beliefs on sensitive issues being deeply held, because people cannot gain accurate information about the beliefs of other people, or for some other reason. But this pattern is worthy of future study.

It is worth considering how our findings relate to other recent work on cultural change. At first glance, our results may seem at odds with the work of Törmos [-@tormosRhythmModernizationHow2021], who emphasizes within-cohort change. But we think there is more common ground that it appears. We replicate his finding that attitudes towards homosexuality have changed considerably across most contexts. While the $\tau$ for this variable tends to be high across  countries, it is not at its theoretical maximum. This means that some within-cohort change is happening everywhere, which he illustrates clearly in his work. 

Consider Törmos's [-@tormosRhythmModernizationHow2021] example of within-cohort changes in beliefs about homosexuality in Sweden, which he argues have been substantial. Our analyses echo his findings in that we find that the relative contribution of between-cohort differences and within-cohort change is fairly equal. In the U.S., $\tau$ is around 0.67, meaning that within-cohort changes play a considerable role in accounting for linear change. Thus, our work replicates some of his main findings: that beliefs about the justifiability of homosexuality display considerable linear change and that within-cohort changes are a key part of that variation. We approach the question of relative importance from a slightly different angle, however; while Törmos focuses on coefficient sizes, we examine the relative explanatory contribution of allowing within-cohort change versus restricting it to zero. We believe that our method compels us to think about the relative explanatory power of each mechanism. Even under the circumstances of steep within-cohort changes, between-cohort differences could remain the primary mechanism of long-term linear change.

Overall, we believe this study contributes to ongoing conversations about the mechanisms of cultural change. We contend that, at the heart of current debates about cultural change, lies the question of the relative explanatory power of within-cohort changes and between-cohort differences. Although we found strong evidence that between-cohort differences and cohort succession are important mechanisms of directional cultural change, we cannot make progress by simply counting the number of variables that seem to be better explained by one process or another. Nor do we simply want to say "it depends" when we consider the relative importance of change mechanisms. Future work will need to look at characteristics of issues themselves to better understand which change mechanisms are more likely to apply. We hope that our investigation of sensitivity as one promising mechanism is a contribution to this endeavor.


\newpage

## References


